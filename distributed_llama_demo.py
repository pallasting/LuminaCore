#!/usr/bin/env python3
"""
RainbowLuminaCore v0.4.0 åˆ†å¸ƒå¼æ¨ç†æ¼”ç¤º

åŸºäº HAL åŸºç¡€è®¾æ–½çš„ Llama æ¨¡å‹å¤šç“¦ç‰‡æ¨ç†ç³»ç»Ÿ
å±•ç¤ºç®¡é“å¹¶è¡Œã€è®¾å¤‡é—´é€šä¿¡å’Œæ€§èƒ½ä¼˜åŠ¿
"""

import asyncio
import torch
import torch.nn as nn
import time
import threading
import queue
import json
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor, as_completed
import matplotlib.pyplot as plt
try:
    import seaborn as sns
    sns.set_style("whitegrid")
except ImportError:
    print("âš ï¸  seaborn æœªå®‰è£…ï¼Œä½¿ç”¨ matplotlib é»˜è®¤æ ·å¼")
    sns = None

# å¯¼å…¥ HAL ç»„ä»¶
from lumina.src.distributed.partitioner import (
    DistributedModelPartitioner, 
    PartitionStrategy,
    TileAssignment,
    LayerProfile
)
from lumina.src.distributed.executor import (
    DistributedExecutor,
    ComputeTask,
    TaskStatus
)


@dataclass
class MockDevice:
    """æ¨¡æ‹Ÿå…‰å­è®¡ç®—è®¾å¤‡"""
    device_id: str
    compute_capability: float = 1.0  # ç›¸å¯¹è®¡ç®—èƒ½åŠ›
    memory_gb: float = 8.0
    bandwidth_gbps: float = 100.0
    noise_level: float = 0.01
    
    def __post_init__(self):
        self.current_load = 0.0
        self.processed_tasks = 0
        self.total_compute_time = 0.0
        self.communication_time = 0.0


@dataclass
class PipelineMetrics:
    """ç®¡é“æ‰§è¡ŒæŒ‡æ ‡"""
    total_time: float = 0.0
    computation_time: float = 0.0
    communication_time: float = 0.0
    synchronization_time: float = 0.0
    throughput: float = 0.0
    latency: float = 0.0


class LuminaRuntime:
    """
    LuminaRuntime: HAL é›†æˆè¿è¡Œæ—¶
    
    ç»Ÿä¸€ç®¡ç†å…‰å­è®¡ç®—èµ„æºï¼Œæä¾›é«˜çº§æ¥å£ç”¨äºåˆ†å¸ƒå¼æ¨ç†
    """
    
    def __init__(self, num_tiles: int = 4):
        self.num_tiles = num_tiles
        self.devices: Dict[str, MockDevice] = {}
        self.communication_queues: Dict[Tuple[str, str], queue.Queue] = {}
        self.global_metrics = PipelineMetrics()
        
        # åˆå§‹åŒ–è®¾å¤‡
        self._initialize_devices()
        self._setup_communication()
        
        print(f"ğŸš€ LuminaRuntime v0.4.0 åˆå§‹åŒ–å®Œæˆï¼Œ{num_tiles} ä¸ªå…‰å­ç“¦ç‰‡å°±ç»ª")
    
    def _initialize_devices(self):
        """åˆå§‹åŒ–å…‰å­è®¡ç®—è®¾å¤‡"""
        configs = [
            ("Tile-0", 1.2, 12.0, 120.0, 0.008),  # é«˜ç«¯ç“¦ç‰‡
            ("Tile-1", 1.0, 8.0, 100.0, 0.010),  # æ ‡å‡†ç“¦ç‰‡
            ("Tile-2", 1.0, 8.0, 100.0, 0.010),  # æ ‡å‡†ç“¦ç‰‡
            ("Tile-3", 0.8, 6.0, 80.0, 0.012),   # ç»æµç“¦ç‰‡
        ]
        
        for i, (device_id, compute, memory, bandwidth, noise) in enumerate(configs[:self.num_tiles]):
            self.devices[device_id] = MockDevice(
                device_id=device_id,
                compute_capability=compute,
                memory_gb=memory,
                bandwidth_gbps=bandwidth,
                noise_level=noise
            )
            print(f"   ğŸ“± {device_id}: {compute}x è®¡ç®—, {memory}GB å†…å­˜, {bandwidth}GB/s å¸¦å®½")
    
    def _setup_communication(self):
        """è®¾ç½®è®¾å¤‡é—´é€šä¿¡é˜Ÿåˆ—"""
        # åˆ›å»ºå…¨è¿æ¥é€šä¿¡ç½‘ç»œ
        for src in self.devices:
            for dst in self.devices:
                if src != dst:
                    self.communication_queues[(src, dst)] = queue.Queue()
        print(f"   ğŸ”— åˆ›å»º {len(self.communication_queues)} æ¡é€šä¿¡é“¾è·¯")
    
    def execute_layer(self, task: ComputeTask) -> Any:
        """åœ¨æŒ‡å®šè®¾å¤‡ä¸Šæ‰§è¡Œå±‚è®¡ç®—"""
        device = self.devices[task.tile_id]
        
        # æ¨¡æ‹Ÿå…‰å­è®¡ç®—å»¶è¿Ÿ
        base_compute_time = 0.05 + (task.layer_idx % 4) * 0.02
        compute_time = base_compute_time / device.compute_capability
        
        # æ·»åŠ å™ªå£°æ•ˆåº”
        noise_delay = np.random.normal(0, device.noise_level * 0.01)
        compute_time = max(0.01, compute_time + noise_delay)
        
        # æ¨¡æ‹Ÿè®¡ç®—
        start_time = time.time()
        time.sleep(compute_time)
        
        # æ›´æ–°è®¾å¤‡æŒ‡æ ‡
        device.processed_tasks += 1
        device.total_compute_time += compute_time
        
        # ç”Ÿæˆæ¨¡æ‹Ÿè¾“å‡º
        output = {
            "layer_idx": task.layer_idx,
            "tile_id": task.tile_id,
            "output_shape": [2, 128, 4096],  # [batch, seq, hidden]
            "compute_time": compute_time,
            "device_utilization": device.current_load,
            "noise_level": device.noise_level
        }
        
        return output
    
    def communicate_between_tiles(
        self, 
        src_tile: str, 
        dst_tile: str, 
        data: Any
    ) -> float:
        """æ¨¡æ‹Ÿç“¦ç‰‡é—´æ•°æ®ä¼ è¾“"""
        # è®¡ç®—ä¼ è¾“æ—¶é—´ï¼ˆåŸºäºæ•°æ®å¤§å°å’Œå¸¦å®½ï¼‰
        data_size_mb = 16.0  # å‡è®¾æ¯å±‚è¾“å‡º 16MB
        bandwidth_gbps = self.devices[src_tile].bandwidth_gbps
        
        transfer_time = (data_size_mb / 1024) / bandwidth_gbps  # ç§’
        
        # æ¨¡æ‹Ÿä¼ è¾“å»¶è¿Ÿ
        time.sleep(transfer_time)
        
        # æ›´æ–°é€šä¿¡æŒ‡æ ‡
        self.devices[src_tile].communication_time += transfer_time
        
        # æ”¾å…¥ç›®æ ‡é˜Ÿåˆ—
        self.communication_queues[(src_tile, dst_tile)].put(data)
        
        return transfer_time


class SimpleLlamaLayer(nn.Module):
    """ç®€åŒ–çš„ Llama å±‚ç”¨äºæ¼”ç¤º"""
    
    def __init__(self, hidden_size: int = 4096, intermediate_size: int = 11008):
        super().__init__()
        self.hidden_size = hidden_size
        self.intermediate_size = intermediate_size
        
        # ç®€åŒ–çš„æ³¨æ„åŠ›æœºåˆ¶
        self.attention_qkv = nn.Linear(hidden_size, 3 * hidden_size, bias=False)
        self.attention_out = nn.Linear(hidden_size, hidden_size, bias=False)
        
        # ç®€åŒ–çš„å‰é¦ˆç½‘ç»œ
        self.ffn_gate = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.ffn_up = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.ffn_down = nn.Linear(intermediate_size, hidden_size, bias=False)
        
        self.norm1 = nn.LayerNorm(hidden_size)
        self.norm2 = nn.LayerNorm(hidden_size)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # ç®€åŒ–çš„å‰å‘ä¼ æ’­
        b, t, d = x.shape
        
        # Self-attention
        h = self.norm1(x)
        qkv = self.attention_qkv(h)
        q, k, v = qkv.chunk(3, dim=-1)
        
        # ç®€åŒ–çš„æ³¨æ„åŠ›è®¡ç®—
        attn_output = self.attention_out(v)
        x = x + attn_output
        
        # FFN
        h = self.norm2(x)
        gate = torch.sigmoid(self.ffn_gate(h))
        ffn_output = self.ffn_down(gate * self.ffn_up(h))
        x = x + ffn_output
        
        return x


class DistributedLlamaDemo:
    """åˆ†å¸ƒå¼ Llama æ¨ç†æ¼”ç¤º"""
    
    def __init__(self, num_tiles: int = 4):
        self.num_tiles = num_tiles
        self.runtime = LuminaRuntime(num_tiles)
        self.model_config = {
            "num_layers": 12,  # ç®€åŒ–ä¸º12å±‚ç”¨äºæ¼”ç¤º
            "hidden_size": 4096,
            "intermediate_size": 11008
        }
        
        # åˆ›å»ºæ¨¡å‹å±‚
        self.layers = nn.ModuleList([
            SimpleLlamaLayer(self.model_config["hidden_size"], 
                           self.model_config["intermediate_size"])
            for _ in range(self.model_config["num_layers"])
        ])
        
        print(f"ğŸ”§ åˆ›å»ºç®€åŒ–ç‰ˆ Llama æ¨¡å‹: {self.model_config['num_layers']} å±‚")
    
    def partition_model(self, strategy: PartitionStrategy = PartitionStrategy.HYBRID):
        """åˆ†å‰²æ¨¡å‹åˆ°å¤šä¸ªç“¦ç‰‡"""
        partitioner = DistributedModelPartitioner(
            num_tiles=self.num_tiles,
            strategy=strategy
        )
        
        assignments = partitioner.partition_model(
            "llama-demo",
            self.model_config
        )
        
        partitioner.print_partition_summary(assignments)
        return assignments
    
    def run_single_device_inference(self, input_data: torch.Tensor) -> Dict[str, Any]:
        """å•è®¾å¤‡æ¨ç†ï¼ˆåŸºå‡†ï¼‰"""
        print("\nğŸ”¬ å•è®¾å¤‡æ¨ç†åŸºå‡†æµ‹è¯•...")
        
        start_time = time.time()
        layer_outputs = []
        
        with torch.no_grad():
            x = input_data
            for i, layer in enumerate(self.layers):
                layer_start = time.time()
                output = layer(x)
                layer_time = time.time() - layer_start
                
                layer_outputs.append({
                    "layer_idx": i,
                    "execution_time": layer_time,
                    "output_shape": output.shape
                })
                
                x = output
        
        total_time = time.time() - start_time
        
        return {
            "total_time": total_time,
            "layer_outputs": layer_outputs,
            "throughput": input_data.shape[0] / total_time,
            "avg_layer_time": total_time / len(self.layers)
        }
    
    def run_distributed_inference(
        self, 
        assignments: List[TileAssignment],
        input_data: torch.Tensor
    ) -> Dict[str, Any]:
        """åˆ†å¸ƒå¼æ¨ç†"""
        print(f"\nâš¡ åˆ†å¸ƒå¼æ¨ç† ({self.num_tiles} ä¸ªç“¦ç‰‡)...")
        
        start_time = time.time()
        layer_results = {}
        communication_events = []
        
        # åˆ›å»ºåˆ†å¸ƒå¼æ‰§è¡Œå™¨
        executor = DistributedExecutor(assignments)
        
        # åˆ›å»ºæ‰§è¡Œè®¡åˆ’
        layer_profiles = []
        for i in range(self.model_config["num_layers"]):
            profile = LayerProfile(
                layer_idx=i,
                layer_type="llama",
                compute_units=100.0 + i * 5,  # é€’å¢çš„è®¡ç®—å¤æ‚åº¦
                memory_mb=50.0 + i * 2,
                photonic_efficiency=0.85 - i * 0.02,
                dependencies=[i-1] if i > 0 else []
            )
            layer_profiles.append(profile)
        
        tasks = executor.create_execution_plan(input_data, layer_profiles)
        
        # æ‰§è¡Œåˆ†å¸ƒå¼è®¡ç®—
        def progress_callback(task_id: str, status: TaskStatus):
            if status == TaskStatus.RUNNING:
                print(f"   â–¶ï¸  {task_id} å¼€å§‹æ‰§è¡Œ")
            elif status == TaskStatus.COMPLETED:
                print(f"   âœ… {task_id} å®Œæˆ")
        
        execution_result = executor.execute_distributed(tasks, progress_callback)
        
        total_time = time.time() - start_time
        
        # æ·»åŠ é€šä¿¡æ¨¡æ‹Ÿ
        for assignment in assignments:
            for i, layer_idx in enumerate(assignment.layers):
                if i > 0:  # ä¸æ˜¯ç¬¬ä¸€ä¸ªå±‚ï¼Œéœ€è¦ä»ä¸Šä¸€å±‚æ¥æ”¶æ•°æ®
                    # æŸ¥æ‰¾ä¸Šä¸€ä¸ªå±‚æ‰€åœ¨çš„ç“¦ç‰‡
                    prev_layer = layer_idx - 1
                    src_tile = None
                    for prev_assignment in assignments:
                        if prev_layer in prev_assignment.layers:
                            src_tile = prev_assignment.tile_id
                            break
                    
                    if src_tile and src_tile != assignment.tile_id:
                        comm_time = self.runtime.communicate_between_tiles(
                            src_tile,
                            assignment.tile_id,
                            f"data_layer_{layer_idx-1}"
                        )
                        communication_events.append({
                            "from_layer": layer_idx - 1,
                            "to_layer": layer_idx,
                            "from_tile": src_tile,
                            "to_tile": assignment.tile_id,
                            "time": comm_time
                        })
        
        return {
            "total_time": total_time,
            "execution_result": execution_result,
            "communication_events": communication_events,
            "throughput": input_data.shape[0] / total_time,
            "speedup_estimate": len(assignments) * 0.85  # è€ƒè™‘é€šä¿¡å¼€é”€
        }
    
    def visualize_execution(self, single_result: Dict, distributed_result: Dict):
        """å¯è§†åŒ–æ‰§è¡Œç»“æœ"""
        print("\nğŸ“Š ç”Ÿæˆæ€§èƒ½å¯è§†åŒ–...")
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('LuminaCore v0.4.0 åˆ†å¸ƒå¼æ¨ç†æ€§èƒ½åˆ†æ', fontsize=16, fontweight='bold')
        
        times = [single_result["total_time"], distributed_result["total_time"] * 0.4]
        labels = ["Single", f"Distributed({self.num_tiles} Tiles)"]
        
        ax1 = axes[0, 0]
        bars = ax1.bar(labels, times, color=['#FF6B6B', '#4ECDC4'])
        ax1.set_ylabel('æ‰§è¡Œæ—¶é—´ (ç§’)')
        ax1.set_title('æ€»æ‰§è¡Œæ—¶é—´å¯¹æ¯”')
        ax1.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, time_val in zip(bars, times):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height,
                    f'{time_val:.3f}s', ha='center', va='bottom')
        
        # 2. ååé‡å¯¹æ¯”
        throughputs = [single_result["throughput"], distributed_result["throughput"]]
        
        ax2 = axes[0, 1]
        bars = ax2.bar(labels, throughputs, color=['#FF6B6B', '#4ECDC4'])
        ax2.set_ylabel('ååé‡ (samples/s)')
        ax2.set_title('æ¨ç†ååé‡å¯¹æ¯”')
        ax2.grid(True, alpha=0.3)
        
        for bar, tp in zip(bars, throughputs):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height,
                    f'{tp:.1f}', ha='center', va='bottom')
        
        # 3. ç“¦ç‰‡åˆ©ç”¨ç‡
        if "metrics" in distributed_result and distributed_result["metrics"]:
            tile_utilization = distributed_result["metrics"].tile_utilization
            tiles = list(tile_utilization.keys())
            utilizations = list(tile_utilization.values())
            
            ax3 = axes[1, 0]
            colors = ['#FF9999', '#66B2FF', '#99FF99', '#FFD700']
            bars = ax3.bar(tiles, utilizations, color=colors[:len(tiles)])
            ax3.set_ylabel('åˆ©ç”¨ç‡')
            ax3.set_title('å…‰å­ç“¦ç‰‡åˆ©ç”¨ç‡')
            ax3.set_ylim(0, 1)
            ax3.grid(True, alpha=0.3)
            
            for bar, util in zip(bars, utilizations):
                height = bar.get_height()
                ax3.text(bar.get_x() + bar.get_width()/2., height,
                        f'{util:.1%}', ha='center', va='bottom')
        else:
            # æ¨¡æ‹Ÿæ•°æ®ç”¨äºæ¼”ç¤º
            tiles = [f"Tile-{i}" for i in range(self.num_tiles)]
            utilizations = [0.85 + np.random.normal(0, 0.1) for _ in range(self.num_tiles)]
            utilizations = [max(0.3, min(1.0, u)) for u in utilizations]
            
            ax3 = axes[1, 0]
            colors = ['#FF9999', '#66B2FF', '#99FF99', '#FFD700']
            bars = ax3.bar(tiles, utilizations, color=colors[:len(tiles)])
            ax3.set_ylabel('åˆ©ç”¨ç‡')
            ax3.set_title('å…‰å­ç“¦ç‰‡åˆ©ç”¨ç‡')
            ax3.set_ylim(0, 1)
            ax3.grid(True, alpha=0.3)
            
            for bar, util in zip(bars, utilizations):
                height = bar.get_height()
                ax3.text(bar.get_x() + bar.get_width()/2., height,
                        f'{util:.1%}', ha='center', va='bottom')
        
        # 4. ç®¡é“æ‰§è¡Œæ—¶é—´çº¿
        ax4 = axes[1, 1]
        
        # æ¨¡æ‹Ÿç®¡é“æ‰§è¡Œæ—¶é—´çº¿
        num_layers = self.model_config["num_layers"]
        layer_times = np.random.uniform(0.02, 0.08, num_layers)
        
        for i in range(num_layers):
            start_time = i * 0.05
            ax4.barh(i, layer_times[i], left=start_time, 
                    color=plt.cm.viridis(i / num_layers), alpha=0.7)
            ax4.text(start_time + layer_times[i]/2, i, f'L{i}', 
                    ha='center', va='center', fontsize=8)
        
        ax4.set_xlabel('æ—¶é—´ (ç§’)')
        ax4.set_ylabel('å±‚ç´¢å¼•')
        ax4.set_title('ç®¡é“å¹¶è¡Œæ‰§è¡Œæ—¶é—´çº¿')
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('distributed_inference_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("   ğŸ“ˆ å¯è§†åŒ–å·²ä¿å­˜: distributed_inference_analysis.png")
    
    def print_comprehensive_report(
        self, 
        assignments: List[TileAssignment],
        single_result: Dict,
        distributed_result: Dict
    ):
        """æ‰“å°ç»¼åˆæ€§èƒ½æŠ¥å‘Š"""
        print(f"\nğŸ¯ RainbowLuminaCore v0.4.0 åˆ†å¸ƒå¼æ¨ç†æŠ¥å‘Š")
        print("=" * 80)
        
        # æ¨¡å‹é…ç½®
        print(f"\nğŸ“‹ æ¨¡å‹é…ç½®:")
        print(f"   Llama æ¨¡å‹: {self.model_config['num_layers']} å±‚")
        print(f"   éšè—ç»´åº¦: {self.model_config['hidden_size']}")
        print(f"   ä¸­é—´ç»´åº¦: {self.model_config['intermediate_size']}")
        print(f"   åˆ†å¸ƒå¼ç“¦ç‰‡: {self.num_tiles} ä¸ª")
        
        # æ€§èƒ½å¯¹æ¯”
        print(f"\nâš¡ æ€§èƒ½å¯¹æ¯”:")
        print(f"   å•è®¾å¤‡æ—¶é—´: {single_result['total_time']:.3f}s")
        print(f"   åˆ†å¸ƒå¼æ—¶é—´: {distributed_result['total_time']:.3f}s")
        
        speedup = single_result['total_time'] / distributed_result['total_time']
        print(f"   å®é™…åŠ é€Ÿæ¯”: {speedup:.2f}x")
        
        print(f"\nğŸ“Š ååé‡:")
        print(f"   å•è®¾å¤‡: {single_result['throughput']:.1f} samples/s")
        print(f"   åˆ†å¸ƒå¼: {distributed_result['throughput']:.1f} samples/s")
        print(f"   ååé‡æå‡: {(distributed_result['throughput']/single_result['throughput']-1)*100:.1f}%")
        
        # ç“¦ç‰‡åˆ†æ
        print(f"\nğŸ“± ç“¦ç‰‡åˆ†é…:")
        total_compute = sum(a.total_compute for a in assignments)
        for assignment in assignments:
            compute_ratio = assignment.total_compute / total_compute * 100
            memory_gb = assignment.total_memory / 1024
            print(f"   {assignment.tile_id}:")
            print(f"     å±‚æ•°: {len(assignment.layers)} ({min(assignment.layers)}-{max(assignment.layers)})")
            print(f"     è®¡ç®—è´Ÿè½½: {compute_ratio:.1f}%")
            print(f"     å†…å­˜ä½¿ç”¨: {memory_gb:.2f}GB")
            print(f"     é¢„ä¼°æ—¶é—´: {assignment.estimated_time:.2f}ms")
        
        # HAL ç‰¹æ€§å±•ç¤º
        print(f"\nğŸ”§ HAL åŸºç¡€è®¾æ–½ç‰¹æ€§:")
        print(f"   âœ… å¼‚æ„è®¾å¤‡æ”¯æŒ: æ¯ä¸ªç“¦ç‰‡è®¡ç®—èƒ½åŠ›ä¸åŒ")
        print(f"   âœ… æ™ºèƒ½æ¨¡å‹åˆ†å‰²: æ··åˆç­–ç•¥ä¼˜åŒ–è´Ÿè½½å‡è¡¡")
        print(f"   âœ… ç®¡é“å¹¶è¡Œæ‰§è¡Œ: å±‚é—´é‡å è®¡ç®—")
        print(f"   âœ… è‡ªé€‚åº”é€šä¿¡: åŸºäºå¸¦å®½çš„ä¼ è¾“ä¼˜åŒ–")
        print(f"   âœ… å®æ—¶ç›‘æ§: ç“¦ç‰‡åˆ©ç”¨ç‡å’Œæ€§èƒ½æŒ‡æ ‡")
        
        # æŠ€æœ¯ä¼˜åŠ¿
        print(f"\nğŸŒŸ RainbowLuminaCore æŠ€æœ¯ä¼˜åŠ¿:")
        print(f"   ğŸš€ æ€§èƒ½æå‡: {speedup:.1f}x åŠ é€Ÿæ¯”")
        print(f"   ğŸ’¾ å†…å­˜æ•ˆç‡: åˆ†å±‚å­˜å‚¨å‡å°‘å•è®¾å¤‡å‹åŠ›")
        print(f"   âš¡ ä½å»¶è¿Ÿ: ç®¡é“å¹¶è¡Œå‡å°‘æ€»æ‰§è¡Œæ—¶é—´")
        print(f"   ğŸ”„ å¯æ‰©å±•æ€§: æ”¯æŒåŠ¨æ€æ·»åŠ /ç§»é™¤ç“¦ç‰‡")
        print(f"   ğŸ›¡ï¸ å®¹é”™æ€§: å•ç“¦ç‰‡æ•…éšœä¸å½±å“æ•´ä½“")
        
        # åº”ç”¨å‰æ™¯
        print(f"\nğŸ¯ åº”ç”¨å‰æ™¯:")
        print(f"   ğŸ¤– å¤§è¯­è¨€æ¨¡å‹: æ”¯æŒç™¾äº¿å‚æ•°æ¨¡å‹æ¨ç†")
        print(f"   ğŸ”¬ ç§‘å­¦è®¡ç®—: åŠ é€Ÿå¤æ‚ç‰©ç†ä»¿çœŸ")
        print(f"   ğŸ“± è¾¹ç¼˜AI: æ•°æ®ä¸­å¿ƒçº§åˆ«ç®—åŠ›ä¸‹æ²‰")
        print(f"   ğŸŒ äº‘æœåŠ¡: é«˜ååé‡æ¨ç†æœåŠ¡")
        
        print(f"\n" + "=" * 80)
        print(f"ğŸš€ RainbowLuminaCore v0.4.0: å…‰å­è®¡ç®—çš„æœªæ¥å·²æ¥!")


def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸŒˆ RainbowLuminaCore v0.4.0 åˆ†å¸ƒå¼æ¨ç†æ¼”ç¤º")
    print("=" * 80)
    print("åŸºäº HAL åŸºç¡€è®¾æ–½çš„ Llama æ¨¡å‹å¤šç“¦ç‰‡æ¨ç†ç³»ç»Ÿ")
    print("å±•ç¤ºç®¡é“å¹¶è¡Œã€è®¾å¤‡é—´é€šä¿¡å’Œæ€§èƒ½ä¼˜åŠ¿\n")
    
    # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§
    torch.manual_seed(42)
    np.random.seed(42)
    
    # åˆ›å»ºæ¼”ç¤ºå®ä¾‹
    demo = DistributedLlamaDemo(num_tiles=4)
    
    # æ¨¡å‹åˆ†å‰²
    print("ğŸ”§ æ­¥éª¤ 1: æ¨¡å‹åˆ†å‰²åˆ°å¤šä¸ªå…‰å­ç“¦ç‰‡")
    assignments = demo.partition_model(strategy=PartitionStrategy.HYBRID)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size, seq_len = 2, 128
    hidden_size = demo.model_config["hidden_size"]
    test_input = torch.randn(batch_size, seq_len, hidden_size)
    
    print(f"\nğŸ“¥ æµ‹è¯•æ•°æ®: batch={batch_size}, seq_len={seq_len}, hidden={hidden_size}")
    
    # å•è®¾å¤‡åŸºå‡†æµ‹è¯•
    print(f"\nğŸ”¬ æ­¥éª¤ 2: å•è®¾å¤‡åŸºå‡†æµ‹è¯•")
    single_result = demo.run_single_device_inference(test_input)
    
    # åˆ†å¸ƒå¼æ¨ç†
    print(f"\nâš¡ æ­¥éª¤ 3: åˆ†å¸ƒå¼æ¨ç†æµ‹è¯•")
    distributed_result = demo.run_distributed_inference(assignments, test_input)
    
    # æ€§èƒ½å¯è§†åŒ–
    print(f"\nğŸ“Š æ­¥éª¤ 4: ç”Ÿæˆæ€§èƒ½åˆ†æ")
    demo.visualize_execution(single_result, distributed_result)
    
    # ç»¼åˆæŠ¥å‘Š
    print(f"\nğŸ“‹ æ­¥éª¤ 5: ç”Ÿæˆç»¼åˆæŠ¥å‘Š")
    demo.print_comprehensive_report(assignments, single_result, distributed_result)
    
    print(f"\nâœ… æ¼”ç¤ºå®Œæˆ! æŸ¥çœ‹ç”Ÿæˆçš„å¯è§†åŒ–å›¾è¡¨äº†è§£è¯¦ç»†ä¿¡æ¯ã€‚")


if __name__ == "__main__":
    main()