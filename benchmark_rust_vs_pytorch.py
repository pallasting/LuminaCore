
import torch
import lumina as lnn
import time
import numpy as np
import torch.nn.functional as F
from lumina.nn import OpticalLinear
import os

def benchmark_optical_vs_linear():
    """å¯¹æ¯”å…‰å­å±‚ä¸ä¼ ç»ŸPyTorchå±‚çš„æ€§èƒ½"""
    
    # å¢å¤§å°ºå¯¸ä»¥ä½“ç° Rust åç«¯ä¼˜åŠ¿
    in_dim = 1024
    out_dim = 1024
    
    # åˆ›å»ºå±‚
    optical_layer = OpticalLinear(in_dim, out_dim, hardware_profile="datacenter_high_precision")
    
    # å®šä¹‰ PyTorch è·¯å¾„çš„æ¨¡æ‹Ÿè¿‡ç¨‹ (åŒ…å«å™ªå£°å’Œé‡åŒ–)
    def pytorch_full_sim(x, layer):
        with torch.no_grad():
            # 1. DAC
            x_q = layer.dac_convert(x)
            # 2. Matmul + Noise
            y = F.linear(x_q, layer.weight, None)
            y_n = layer.noise_model.apply_noise(y, True)
            # 3. ADC
            y_out = layer.adc_convert(y_n)
            return y_out

    # æµ‹è¯•æ•°æ®
    batch_sizes = [32, 64, 128]
    num_iterations = 20
    
    print("ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯• (è®­ç»ƒæ¨¡å¼ï¼šåŒ…å«å™ªå£°ä¸é‡åŒ–)")
    print("=" * 70)
    print(f"{'Batch Size':<12} {'Rust Fused (ms)':<18} {'PyTorch Full (ms)':<18} {'Speedup':<10}")
    print("-" * 70)
    
    for batch_size in batch_sizes:
        x = torch.randn(batch_size, in_dim)
        
        # 1. Rust èåˆç®—å­æµ‹è¯•
        # ç¡®ä¿åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ï¼Œä½†æˆ‘ä»¬æ‰‹åŠ¨è°ƒç”¨ _forward_rust æ¥æµ‹è¯•å®ƒ
        # å› ä¸º OpticalLinear.forward ç›®å‰åœ¨ training=True æ—¶ä¼šå›é€€
        os.environ["LUMINA_USE_RUST"] = "1"
        torch.cuda.synchronize() if torch.cuda.is_available() else None
        start = time.time()
        for _ in range(num_iterations):
            y_rust = optical_layer._forward_rust(x)
        torch.cuda.synchronize() if torch.cuda.is_available() else None
        rust_time = (time.time() - start) * 1000 / num_iterations
        
        # 2. PyTorch å…¨æ¨¡æ‹Ÿè·¯å¾„æµ‹è¯•
        torch.cuda.synchronize() if torch.cuda.is_available() else None
        start = time.time()
        for _ in range(num_iterations):
            y_torch = pytorch_full_sim(x, optical_layer)
        torch.cuda.synchronize() if torch.cuda.is_available() else None
        torch_time = (time.time() - start) * 1000 / num_iterations
        
        speedup = torch_time / rust_time
        print(f"{batch_size:<12} {rust_time:<18.3f} {torch_time:<18.3f} {speedup:<10.2f}x")
    
    print("\nğŸ‰ åŸºå‡†æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    benchmark_optical_vs_linear()
