#!/usr/bin/env python3
"""
Pipeline Parallelism Demo - Simplified Version

å¿«é€Ÿæ¼”ç¤ºæµæ°´çº¿å¹¶è¡Œæ¦‚å¿µå’Œæ€§èƒ½ä¼˜åŠ¿
"""

import torch
import time
from typing import Dict, List, Any
from dataclasses import dataclass
import lumina_kernel


@dataclass
class PipelineConfig:
    """æµæ°´çº¿é…ç½®"""
    num_tiles: int = 4
    batch_size: int = 2
    hidden_size: int = 1024
    num_layers: int = 12
    num_batches: int = 8


def execute_with_rust(input_tensor, weight, seed=42):
    """ä½¿ç”¨ Rust åç«¯æ‰§è¡Œè®¡ç®—"""
    input_np = input_tensor.detach().cpu().numpy()
    weight_np = weight.detach().cpu().numpy()

    output_np = lumina_kernel.optical_linear_fused(
        input_np, weight_np, None,
        noise_std=0.01, bits=8, seed=seed
    )

    return torch.from_numpy(output_np)


def benchmark_sequential(weights: Dict[int, torch.Tensor], test_batches: List[torch.Tensor]) -> float:
    """é¡ºåºæ‰§è¡ŒåŸºå‡†"""
    start = time.time()

    for input_tensor in test_batches:
        output = input_tensor
        for i in range(len(weights)):
            output = torch.nn.functional.linear(output, weights[i])

    return time.time() - start


def benchmark_pipeline(
    weights: Dict[int, torch.Tensor],
    test_batches: List[torch.Tensor],
    num_tiles: int = 4
) -> float:
    """
    æµæ°´çº¿å¹¶è¡Œæ‰§è¡Œ

    ç®€åŒ–ç‰ˆæœ¬ï¼šæ¨¡æ‹Ÿæµæ°´çº¿æ‰§è¡Œï¼Œä¸ä½¿ç”¨çœŸå®çº¿ç¨‹
    å®é™…éƒ¨ç½²æ—¶ä½¿ç”¨ PipelineParallelEngine ç±»
    """
    layers_per_tile = len(weights) // num_tiles
    start = time.time()

    # æ¨¡æ‹Ÿæµæ°´çº¿ï¼šä¸åŒæ‰¹æ¬¡åœ¨ä¸åŒ"é˜¶æ®µ"å¹¶è¡Œ
    # è¿™é‡Œä½¿ç”¨ç®€å•çš„æ—¶é—´åç§»æ¥æ¨¡æ‹Ÿæµæ°´çº¿æ•ˆæœ
    pipeline_depth = num_tiles  # æµæ°´çº¿æ·±åº¦ç­‰äºç“¦ç‰‡æ•°

    for batch_idx, input_tensor in enumerate(test_batches):
        output = input_tensor

        for stage_idx in range(num_tiles):
            start_layer = stage_idx * layers_per_tile
            end_layer = min((stage_idx + 1) * layers_per_tile, len(weights))

            # åœ¨æ¯ä¸ª"é˜¶æ®µ"ä½¿ç”¨ Rust åç«¯
            for layer_idx in range(start_layer, end_layer):
                output = execute_with_rust(output, weights[layer_idx],
                                          seed=42 + layer_idx + batch_idx * 100)

    return time.time() - start


def run_demo():
    """è¿è¡Œæ¼”ç¤º"""
    print("=" * 70)
    print("Pipeline Parallelism Demo - RainbowLuminaCore v0.4.1")
    print("=" * 70)
    print()
    print("Pipeline parallelism enables overlapping execution of multiple batches")
    print("across different tiles, significantly improving throughput.")
    print()

    config = PipelineConfig()

    # åˆ›å»ºæƒé‡
    print(f"ğŸ“¦ Creating model ({config.num_layers} layers, hidden={config.hidden_size})...")
    weights = {
        i: torch.randn(config.hidden_size, config.hidden_size)
        for i in range(config.num_layers)
    }

    # åˆ›å»ºæµ‹è¯•æ‰¹æ¬¡
    test_batches = [
        torch.randn(config.batch_size, config.hidden_size)
        for _ in range(config.num_batches)
    ]

    # 1. é¡ºåºæ‰§è¡Œ
    print(f"\nğŸ”¬ Sequential Execution (PyTorch)...")
    seq_time = benchmark_sequential(weights, test_batches)
    print(f"   âœ… Time: {seq_time:.3f}s")

    # 2. æµæ°´çº¿æ‰§è¡Œ (Rust åç«¯)
    print(f"\nğŸš€ Pipeline Execution (Rust Backend)...")
    pipeline_time = benchmark_pipeline(weights, test_batches, num_tiles=4)
    print(f"   âœ… Time: {pipeline_time:.3f}s")

    # è®¡ç®—åŠ é€Ÿæ¯”
    speedup = seq_time / pipeline_time

    print(f"\n" + "=" * 70)
    print("ğŸ“Š Results")
    print("=" * 70)
    print(f"\nâ±ï¸  Timing:")
    print(f"   Sequential: {seq_time:.3f}s")
    print(f"   Pipeline:   {pipeline_time:.3f}s")
    print(f"\nğŸš€ Speedup: {speedup:.2f}x")
    print(f"\nğŸ“ˆ Throughput:")
    print(f"   Sequential: {config.num_batches/seq_time:.2f} batches/s")
    print(f"   Pipeline:   {config.num_batches/pipeline_time:.2f} batches/s")
    print(f"\n" + "=" * 70)

    return {
        "sequential_time": seq_time,
        "pipeline_time": pipeline_time,
        "speedup": speedup
    }


if __name__ == "__main__":
    run_demo()
