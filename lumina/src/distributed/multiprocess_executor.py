#!/usr/bin/env python3
"""
Multi-Process Executor Implementation

ä½¿ç”¨ Python multiprocessing æ¨¡å—ç»•è¿‡ GIL é™åˆ¶ï¼Œå®ç°çœŸæ­£çš„å¹¶è¡Œè®¡ç®—ã€‚
æ¯ä¸ªè¿›ç¨‹ä»£è¡¨ä¸€ä¸ªç‹¬ç«‹çš„å…‰å­è®¡ç®—ç“¦ç‰‡ (Tile)ã€‚
"""

import torch
import numpy as np
import time
import multiprocessing as mp
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
import os

# å¯¼å…¥ Rust å†…æ ¸ (åœ¨å­è¿›ç¨‹ä¸­éœ€è¦é‡æ–°å¯¼å…¥)
import lumina_kernel

@dataclass
class TaskPayload:
    """ä»»åŠ¡è´Ÿè½½"""
    task_id: str
    layer_idx: int
    input_data: np.ndarray  # ä½¿ç”¨ numpy ä¼ é€’ï¼Œé¿å… torch åºåˆ—åŒ–é—®é¢˜
    weight: np.ndarray
    bias: Optional[np.ndarray]
    noise_std: float
    bits: int
    seed: int

@dataclass
class TaskResult:
    """ä»»åŠ¡ç»“æœ"""
    task_id: str
    output_data: np.ndarray
    execution_time: float
    process_id: int
    error: Optional[str] = None

def _worker_process(
    tile_id: str,
    task_queue: mp.Queue,
    result_queue: mp.Queue,
    device_config: Dict[str, Any]
):
    """
    å·¥ä½œè¿›ç¨‹å‡½æ•° (ç‹¬ç«‹è¿è¡Œåœ¨å•ç‹¬çš„è¿›ç¨‹ä¸­)
    """
    pid = os.getpid()
    print(f"   [Worker-{tile_id}] Started (PID: {pid})")
    
    # åœ¨è¿›ç¨‹å†…åˆå§‹åŒ– Rust æ¨¡æ‹Ÿè®¾å¤‡
    # æ³¨æ„ï¼šæ¯ä¸ªè¿›ç¨‹æœ‰è‡ªå·±çš„å†…å­˜ç©ºé—´ï¼Œæ‰€ä»¥ Rust ç«¯çš„é™æ€å˜é‡æ˜¯éš”ç¦»çš„
    lumina_kernel.create_mock_device(f"Device-{tile_id}", 8 * 1024**3)
    
    while True:
        try:
            task: Optional[TaskPayload] = task_queue.get()
            if task is None:  # ç»ˆæ­¢ä¿¡å·
                break
            
            start_time = time.time()
            
            # è°ƒç”¨ Rust å†…æ ¸æ‰§è¡Œè®¡ç®—
            # æ³¨æ„ï¼šè¿™é‡Œçš„æ•°æ®å·²ç»æ˜¯ numpy æ•°ç»„
            output_np = lumina_kernel.optical_linear_fused(
                task.input_data,
                task.weight,
                task.bias,
                task.noise_std,
                task.bits,
                task.seed
            )
            
            exec_time = time.time() - start_time
            
            result = TaskResult(
                task_id=task.task_id,
                output_data=output_np,
                execution_time=exec_time,
                process_id=pid
            )
            
            result_queue.put(result)
            
        except Exception as e:
            import traceback
            traceback.print_exc()
            result_queue.put(TaskResult(
                task_id=task.task_id if 'task' in locals() else "unknown",
                output_data=np.array([]),
                execution_time=0.0,
                process_id=pid,
                error=str(e)
            ))
            
    print(f"   [Worker-{tile_id}] Stopped")

class MultiProcessExecutor:
    """
    å¤šè¿›ç¨‹æ‰§è¡Œå™¨
    
    ç‰¹ç‚¹:
    - çœŸæ­£çš„å¹¶è¡Œæ‰§è¡Œ (ç»•è¿‡ GIL)
    - è¿›ç¨‹éš”ç¦»ï¼Œæ¨¡æ‹Ÿç‹¬ç«‹çš„è®¡ç®—èŠ‚ç‚¹
    - åŸºäº Queue çš„é€šä¿¡
    """
    
    def __init__(self, num_tiles: int = 4):
        self.num_tiles = num_tiles
        self.processes: List[mp.Process] = []
        self.task_queues: List[mp.Queue] = []
        self.result_queues: List[mp.Queue] = []
        self.running = False
        
        print(f"ğŸš€ Initializing MultiProcessExecutor with {num_tiles} tiles")
        self._start_processes()
        
    def _start_processes(self):
        """å¯åŠ¨å·¥ä½œè¿›ç¨‹"""
        self.running = True
        for i in range(self.num_tiles):
            task_q = mp.Queue()
            result_q = mp.Queue()
            
            p = mp.Process(
                target=_worker_process,
                args=(f"Tile-{i}", task_q, result_q, {})
            )
            p.daemon = True
            p.start()
            
            self.processes.append(p)
            self.task_queues.append(task_q)
            self.result_queues.append(result_q)
            
    def execute_batch_parallel(
        self, 
        layers_data: List[Dict[str, Any]]
    ) -> List[np.ndarray]:
        """
        å¹¶è¡Œæ‰§è¡Œä¸€æ‰¹å±‚ä»»åŠ¡
        
        Args:
            layers_data: ä»»åŠ¡åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« input, weight, etc.
            
        Returns:
            ç»“æœåˆ—è¡¨
        """
        num_tasks = len(layers_data)
        if num_tasks == 0:
            return []
            
        # åˆ†å‘ä»»åŠ¡
        for i, data in enumerate(layers_data):
            tile_idx = i % self.num_tiles
            
            # å‡†å¤‡æ•°æ® (è½¬ä¸º numpy)
            input_np = data['input']
            if isinstance(input_np, torch.Tensor):
                input_np = input_np.detach().cpu().numpy()
                
            weight_np = data['weight']
            if isinstance(weight_np, torch.Tensor):
                weight_np = weight_np.detach().cpu().numpy()
                
            bias_np = data.get('bias')
            if isinstance(bias_np, torch.Tensor):
                bias_np = bias_np.detach().cpu().numpy()
                
            payload = TaskPayload(
                task_id=f"Task-{i}",
                layer_idx=i,
                input_data=input_np,
                weight=weight_np,
                bias=bias_np,
                noise_std=data.get('noise_std', 0.01),
                bits=data.get('bits', 8),
                seed=42 + i
            )
            
            self.task_queues[tile_idx].put(payload)
            
        # æ”¶é›†ç»“æœ
        results = [None] * num_tasks
        collected = 0
        
        # ç®€å•çš„è½®è¯¢æ”¶é›† (å®é™…åº”è¯¥ä½¿ç”¨æ›´å¤æ‚çš„å¼‚æ­¥æœºåˆ¶)
        # è¿™é‡Œä¸ºäº†æ¼”ç¤ºç®€å•ï¼Œæˆ‘ä»¬å‡è®¾æŒ‰é¡ºåºæ”¶é›†ï¼Œæˆ–è€…ä½¿ç”¨ ID æ˜ å°„
        pending_tiles = list(range(min(num_tasks, self.num_tiles))) # æ´»è·ƒçš„ tiles
        tile_task_counts = [0] * self.num_tiles
        for i in range(num_tasks):
            tile_task_counts[i % self.num_tiles] += 1
            
        for tile_idx in range(self.num_tiles):
            count = tile_task_counts[tile_idx]
            for _ in range(count):
                res = self.result_queues[tile_idx].get()
                if res.error:
                    print(f"âŒ Error in task {res.task_id}: {res.error}")
                else:
                    # è§£æ task_id è·å–ç´¢å¼• "Task-{i}"
                    idx = int(res.task_id.split('-')[1])
                    results[idx] = res.output_data
                    
        return results

    def shutdown(self):
        """å…³é—­æ‰§è¡Œå™¨"""
        print("ğŸ›‘ Shutting down MultiProcessExecutor...")
        for q in self.task_queues:
            q.put(None)
            
        for p in self.processes:
            p.join(timeout=1.0)
            if p.is_alive():
                p.terminate()
        
        self.running = False
        print("âœ… Shutdown complete")

def benchmark_multiprocess():
    """åŸºå‡†æµ‹è¯•ï¼šå¤šçº¿ç¨‹ vs å¤šè¿›ç¨‹"""
    print("=" * 60)
    print("Multi-Process vs Multi-Thread Benchmark")
    print("=" * 60)
    
    num_layers = 16
    hidden_size = 2048
    batch_size = 8
    
    # å‡†å¤‡æ•°æ®
    weights = [np.random.randn(hidden_size, hidden_size).astype(np.float32) for _ in range(num_layers)]
    inputs = [np.random.randn(batch_size, hidden_size).astype(np.float32) for _ in range(num_layers)]
    
    tasks = []
    for i in range(num_layers):
        tasks.append({
            "input": inputs[i],
            "weight": weights[i],
            "layer_idx": i
        })
        
    # 1. å¤šè¿›ç¨‹æµ‹è¯•
    print("\nğŸš€ Multi-Process Execution...")
    mp_executor = MultiProcessExecutor(num_tiles=4)
    
    start = time.time()
    mp_results = mp_executor.execute_batch_parallel(tasks)
    mp_time = time.time() - start
    
    mp_executor.shutdown()
    print(f"   âœ… Time: {mp_time:.4f}s")
    
    # 2. å¤šçº¿ç¨‹æµ‹è¯• (ä½¿ç”¨ä¹‹å‰çš„ ThreadPoolExecutor)
    print("\nğŸ§µ Multi-Thread Execution (GIL limited)...")
    from concurrent.futures import ThreadPoolExecutor
    
    def thread_worker(task):
        return lumina_kernel.optical_linear_fused(
            task['input'], task['weight'], None, 0.01, 8, 42
        )
        
    start = time.time()
    with ThreadPoolExecutor(max_workers=4) as executor:
        list(executor.map(thread_worker, tasks))
    mt_time = time.time() - start
    print(f"   âœ… Time: {mt_time:.4f}s")
    
    # 3. ä¸²è¡ŒåŸºå‡†
    print("\nğŸ¢ Sequential Execution...")
    start = time.time()
    for task in tasks:
        thread_worker(task)
    seq_time = time.time() - start
    print(f"   âœ… Time: {seq_time:.4f}s")
    
    print("\n" + "=" * 60)
    print("ğŸ“Š Comparison Results")
    print("=" * 60)
    print(f"   Sequential:   {seq_time:.4f}s (1.00x)")
    print(f"   Multi-Thread: {mt_time:.4f}s ({seq_time/mt_time:.2f}x)")
    print(f"   Multi-Process:{mp_time:.4f}s ({seq_time/mp_time:.2f}x)")
    print("\n   Note: Multi-process overhead (IPC) might affect small tasks.")
    print("   For large matrix operations, MP should win significantly.")

if __name__ == "__main__":
    #å¿…é¡»è¦åŠ è¿™ä¸ªï¼Œå¦åˆ™å¤šè¿›ç¨‹åœ¨æŸäº›ç³»ç»Ÿä¼šæŠ¥é”™
    mp.set_start_method('spawn', force=True)
    benchmark_multiprocess()
