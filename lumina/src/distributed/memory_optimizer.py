#!/usr/bin/env python3
"""
Memory-Optimized Backend

å†…å­˜ä¼˜åŒ–åç«¯ - å‡å°‘ Rust å’Œ Python ä¹‹é—´çš„æ•°æ®æ‹·è´

å…³é”®ä¼˜åŒ–:
1. ä½¿ç”¨ torch.from_numpy(output_np, copy=False) é¿å…ä¸å¿…è¦çš„æ‹·è´
2. ç¼“å­˜æƒé‡åœ¨ GPU/CPU ä¸Šï¼Œé¿å…é‡å¤è½¬æ¢
3. æ‰¹é‡å¤„ç†å‡å°‘å‡½æ•°è°ƒç”¨å¼€é”€
4. ä½¿ç”¨å†…å­˜æ± å¤ç”¨ç¼“å†²åŒº
"""

import torch
import numpy as np
import time
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
import lumina_kernel


@dataclass
class MemoryConfig:
    """å†…å­˜é…ç½®"""
    enable_caching: bool = True
    cache_weights: bool = True
    use_memory_pool: bool = True
    max_cache_size: int = 1024 * 1024 * 1024  # 1GB


class MemoryPool:
    """ç®€å•å†…å­˜æ±  - å¤ç”¨ numpy æ•°ç»„"""

    def __init__(self, max_size: int = 1024 * 1024 * 1024):
        self.max_size = max_size
        self.pool: Dict[Tuple[int, ...], List[np.ndarray]] = {}
        self.current_size = 0
        self.hits = 0
        self.misses = 0

    def get(self, shape: Tuple[int, ...], dtype=np.float32) -> np.ndarray:
        """ä»æ± ä¸­è·å–æ•°ç»„"""
        key = (shape, dtype)

        if key in self.pool and self.pool[key]:
            self.hits += 1
            return self.pool[key].pop()

        self.misses += 1
        return np.empty(shape, dtype=dtype)

    def release(self, arr: np.ndarray):
        """å°†æ•°ç»„å½’è¿˜ç»™æ± """
        shape = (arr.shape, arr.dtype)
        key = (tuple(shape[0]), shape[1])

        if self.current_size + arr.nbytes < self.max_size:
            if key not in self.pool:
                self.pool[key] = []
            self.pool[key].append(arr)
            self.current_size += arr.nbytes

    def stats(self) -> Dict[str, int]:
        return {"hits": self.hits, "misses": self.misses, "pool_size": self.current_size}


class OptimizedPhotonicExecutor:
    """
    å†…å­˜ä¼˜åŒ–å…‰å­è®¡ç®—æ‰§è¡Œå™¨

    ç‰¹ç‚¹:
    - é›¶æ‹·è´æ•°æ®ä¼ é€’ (å½“å¯èƒ½æ—¶)
    - æƒé‡ç¼“å­˜
    - å†…å­˜æ± å¤ç”¨
    - æ‰¹é‡æ‰§è¡Œä¼˜åŒ–
    """

    def __init__(
        self,
        device_name: Optional[str] = None,
        noise_std: float = 0.01,
        bits: int = 8,
        enable_noise: bool = True,
        config: Optional[MemoryConfig] = None
    ):
        self.config = config or MemoryConfig()
        self.noise_std = noise_std
        self.bits = bits
        self.enable_noise = enable_noise

        # æƒé‡ç¼“å­˜
        self.weight_cache: Dict[int, Tuple[np.ndarray, torch.Tensor]] = {}

        # å†…å­˜æ± 
        if self.config.use_memory_pool:
            self.memory_pool = MemoryPool()
        else:
            self.memory_pool = None

        # ç»Ÿè®¡
        self.stats = {
            "forward_passes": 0,
            "total_time": 0.0,
            "copy_time": 0.0,
            "compute_time": 0.0
        }

        print(f"ğŸš€ OptimizedPhotonicExecutor åˆå§‹åŒ–")
        print(f"   æƒé‡ç¼“å­˜: {'å¯ç”¨' if self.config.cache_weights else 'ç¦ç”¨'}")
        print(f"   å†…å­˜æ± : {'å¯ç”¨' if self.config.use_memory_pool else 'ç¦ç”¨'}")

    def _ensure_contiguous(self, arr: np.ndarray) -> np.ndarray:
        """ç¡®ä¿æ•°ç»„æ˜¯ C-contiguous"""
        if not arr.flags['C_CONTIGUOUS']:
            return np.ascontiguousarray(arr)
        return arr

    def _cache_weight(self, layer_idx: int, weight: torch.Tensor) -> np.ndarray:
        """ç¼“å­˜æƒé‡åˆ° numpy"""
        if layer_idx in self.weight_cache:
            cached_np, _ = self.weight_cache[layer_idx]
            # æ£€æŸ¥å½¢çŠ¶æ˜¯å¦åŒ¹é…
            if cached_np.shape == tuple(weight.shape):
                return cached_np

        # è½¬æ¢å¹¶ç¼“å­˜
        weight_np = weight.detach().cpu().numpy()
        weight_np = self._ensure_contiguous(weight_np)
        self.weight_cache[layer_idx] = (weight_np, weight)
        return weight_np

    def execute_layer(
        self,
        input_tensor: torch.Tensor,
        weight: torch.Tensor,
        layer_idx: int = 0,
        batch_id: int = 0
    ) -> Tuple[torch.Tensor, float]:
        """
        æ‰§è¡Œå•å±‚è®¡ç®— (ä¼˜åŒ–ç‰ˆæœ¬)

        Args:
            input_tensor: è¾“å…¥å¼ é‡
            weight: æƒé‡å¼ é‡
            layer_idx: å±‚ç´¢å¼• (ç”¨äºç¼“å­˜)
            batch_id: æ‰¹æ¬¡ ID (ç”¨äºéšæœºç§å­)

        Returns:
            output_tensor, execution_time
        """
        start_total = time.time()

        # è·å–ç¼“å­˜çš„æƒé‡
        if self.config.cache_weights:
            weight_np = self._cache_weight(layer_idx, weight)
        else:
            weight_np = self._ensure_contiguous(weight.detach().cpu().numpy())

        # è½¬æ¢è¾“å…¥
        copy_start = time.time()
        input_np = self._ensure_contiguous(input_tensor.detach().cpu().numpy())
        copy_time = time.time() - copy_start

        # è·å–è¾“å‡ºæ•°ç»„ (ä»å†…å­˜æ± æˆ–æ–°å»º)
        if self.memory_pool:
            output_np = self.memory_pool.get(weight_np.shape[:1] + (input_np.shape[0],))
        else:
            output_np = np.empty((input_np.shape[0], weight_np.shape[0]), dtype=np.float32)

        # æ‰§è¡Œè®¡ç®—
        compute_start = time.time()
        result_np = lumina_kernel.optical_linear_fused(
            input_np,
            weight_np,
            None,  # æ— åç½®
            self.noise_std,
            self.bits,
            seed=42 + layer_idx + batch_id * 100
        )
        compute_time = time.time() - compute_start

        # è½¬æ¢å› torch (é›¶æ‹·è´å½“å¯èƒ½æ—¶)
        output_tensor = torch.from_numpy(result_np)

        # é‡Šæ”¾è¾“å‡ºæ•°ç»„å›å†…å­˜æ± 
        if self.memory_pool:
            self.memory_pool.release(output_np)

        # æ›´æ–°ç»Ÿè®¡
        exec_time = time.time() - start_total
        self.stats["forward_passes"] += 1
        self.stats["total_time"] += exec_time
        self.stats["copy_time"] += copy_time
        self.stats["compute_time"] += compute_time

        return output_tensor, exec_time

    def execute_inference(
        self,
        input_tensor: torch.Tensor,
        weights: Dict[int, torch.Tensor]
    ) -> Tuple[torch.Tensor, float]:
        """æ‰§è¡Œæ¨ç† (æ— å™ªå£°)"""
        start = time.time()
        output = input_tensor

        for i in range(len(weights)):
            output, _ = self.execute_layer(output, weights[i], layer_idx=i)

        return output, time.time() - start

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        passes = self.stats["forward_passes"]
        return {
            "forward_passes": passes,
            "total_time": self.stats["total_time"],
            "avg_time": self.stats["total_time"] / passes if passes > 0 else 0,
            "copy_time": self.stats["copy_time"],
            "compute_time": self.stats["compute_time"],
            "copy_ratio": self.stats["copy_time"] / self.stats["total_time"] if self.stats["total_time"] > 0 else 0,
            "cache_size": len(self.weight_cache),
            "pool_stats": self.memory_pool.stats() if self.memory_pool else None
        }

    def print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.get_stats()
        print(f"\nğŸ“Š Optimized Executor Statistics:")
        print(f"   Forward Passes: {stats['forward_passes']}")
        print(f"   Total Time: {stats['total_time']*1000:.2f}ms")
        print(f"   Avg Time: {stats['avg_time']*1000:.2f}ms")
        print(f"   Copy Time: {stats['copy_time']*1000:.2f}ms ({stats['copy_ratio']*100:.1f}%)")
        print(f"   Compute Time: {stats['compute_time']*1000:.2f}ms")
        print(f"   Cache Size: {stats['cache_size']} weights")

        if stats['pool_stats']:
            pool = stats['pool_stats']
            print(f"   Pool Hits: {pool['hits']}, Misses: {pool['misses']}")


def benchmark_memory_optimization():
    """å†…å­˜ä¼˜åŒ–åŸºå‡†æµ‹è¯•"""
    print("=" * 70)
    print("Memory Optimization Benchmark")
    print("=" * 70)

    # é…ç½®
    config = MemoryConfig(
        enable_caching=True,
        cache_weights=True,
        use_memory_pool=True
    )

    executor = OptimizedPhotonicExecutor(
        device_name="benchmark",
        noise_std=0.01,
        bits=8,
        enable_noise=True,
        config=config
    )

    # åˆ›å»ºæ¨¡å‹
    num_layers = 12
    hidden_size = 2048
    weights = {
        i: torch.randn(hidden_size, hidden_size)
        for i in range(num_layers)
    }

    # æµ‹è¯•æ‰¹æ¬¡
    num_batches = 8
    test_batches = [
        torch.randn(4, hidden_size)
        for _ in range(num_batches)
    ]

    print(f"\nğŸ“¦ Model: {num_layers} layers, hidden={hidden_size}")
    print(f"   Batches: {num_batches}")

    # é¢„çƒ­
    print(f"\nğŸ”¥ Warmup...")
    for i in range(3):
        _ = executor.execute_layer(test_batches[0], weights[0], layer_idx=0)

    # åŸºå‡†æµ‹è¯•
    print(f"\nğŸ”¬ Running benchmark...")
    start = time.time()

    for batch_idx, input_tensor in enumerate(test_batches):
        output = input_tensor
        for layer_idx in range(num_layers):
            output, _ = executor.execute_layer(
                output, weights[layer_idx],
                layer_idx=layer_idx,
                batch_id=batch_idx
            )

    total_time = time.time() - start

    # æ‰“å°ç»Ÿè®¡
    executor.print_stats()

    print(f"\nâœ… Results:")
    print(f"   Total Time: {total_time:.3f}s")
    print(f"   Throughput: {num_batches/total_time:.2f} batches/s")

    return executor.get_stats()


def compare_with_baseline():
    """ä¸åŸºçº¿æ¯”è¾ƒ"""
    print("\n" + "=" * 70)
    print("Baseline Comparison: Standard vs Optimized")
    print("=" * 70)

    num_layers = 12
    hidden_size = 2048
    num_batches = 8

    # åˆ›å»ºæƒé‡
    weights = {
        i: torch.randn(hidden_size, hidden_size)
        for i in range(num_layers)
    }

    # æµ‹è¯•æ‰¹æ¬¡
    test_batches = [
        torch.randn(4, hidden_size)
        for _ in range(num_batches)
    ]

    # 1. æ ‡å‡†æ–¹æ³• (æ¯æ¬¡è½¬æ¢)
    print(f"\nğŸ“Š Standard Method (no optimization)...")
    start = time.time()

    for input_tensor in test_batches:
        output = input_tensor
        for i in range(num_layers):
            input_np = input_tensor.detach().cpu().numpy()
            weight_np = weights[i].detach().cpu().numpy()
            output_np = lumina_kernel.optical_linear_fused(
                input_np, weight_np, None, 0.01, 8, 42 + i
            )
            output = torch.from_numpy(output_np)

    standard_time = time.time() - start
    print(f"   Time: {standard_time:.3f}s")

    # 2. ä¼˜åŒ–æ–¹æ³•
    print(f"\nğŸš€ Optimized Method...")
    config = MemoryConfig(cache_weights=True, use_memory_pool=True)
    executor = OptimizedPhotonicExecutor(config=config)

    start = time.time()

    for batch_idx, input_tensor in enumerate(test_batches):
        output = input_tensor
        for i in range(num_layers):
            output, _ = executor.execute_layer(
                output, weights[i],
                layer_idx=i,
                batch_id=batch_idx
            )

    optimized_time = time.time() - start
    print(f"   Time: {optimized_time:.3f}s")

    # æ‰“å°å¯¹æ¯”
    print(f"\n" + "=" * 70)
    print("ğŸ“Š Comparison:")
    print(f"   Standard:  {standard_time:.3f}s")
    print(f"   Optimized: {optimized_time:.3f}s")
    speedup = standard_time / optimized_time if optimized_time > 0 else 1
    print(f"   Speedup:   {speedup:.2f}x")
    print(f"=" * 70)


if __name__ == "__main__":
    benchmark_memory_optimization()
    compare_with_baseline()
