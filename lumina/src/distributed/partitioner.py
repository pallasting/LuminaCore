"""
åˆ†å¸ƒå¼å…‰å­è®¡ç®—æ¨¡å‹åˆ†å‰²å™¨

å°† Llama æ¨¡å‹æ™ºèƒ½åˆ†å‰²åˆ°å¤šä¸ªå…‰å­è®¡ç®—ç“¦ç‰‡ä¸Š
"""

import math
import torch
import torch.nn as nn
from typing import List, Dict, Tuple, Any
from dataclasses import dataclass
from enum import Enum


class PartitionStrategy(Enum):
    """æ¨¡å‹åˆ†å‰²ç­–ç•¥"""
    BY_LAYERS = "by_layers"           # æŒ‰ Transformer å±‚å‡åŒ€åˆ†å‰²
    BY_COMPUTE = "by_compute"         # æŒ‰è®¡ç®—å¤æ‚åº¦åˆ†å‰²
    BY_MEMORY = "by_memory"           # æŒ‰å†…å­˜éœ€æ±‚åˆ†å‰²
    HYBRID = "hybrid"                  # æ··åˆç­–ç•¥


@dataclass
class LayerProfile:
    """å±‚æ€§èƒ½é…ç½®æ–‡ä»¶"""
    layer_idx: int
    layer_type: str
    compute_units: float  # ç›¸å¯¹è®¡ç®—å•å…ƒæ•°
    memory_mb: float      # å†…å­˜éœ€æ±‚ (MB)
    photonic_efficiency: float  # å…‰å­è®¡ç®—æ•ˆç‡ (0-1)
    dependencies: List[int]  # ä¾èµ–çš„å±‚ç´¢å¼•


@dataclass 
class TileAssignment:
    """ç“¦ç‰‡åˆ†é…é…ç½®"""
    tile_id: str
    device_id: str
    layers: List[int]           # åˆ†é…çš„å±‚ç´¢å¼•
    total_compute: float        # æ€»è®¡ç®—è´Ÿè½½
    total_memory: float         # æ€»å†…å­˜éœ€æ±‚
    estimated_time: float       # é¢„ä¼°æ‰§è¡Œæ—¶é—´ (ms)


class DistributedModelPartitioner:
    """åˆ†å¸ƒå¼æ¨¡å‹åˆ†å‰²å™¨"""
    
    def __init__(
        self,
        num_tiles: int = 4,
        strategy: PartitionStrategy = PartitionStrategy.HYBRID,
        memory_limit_per_tile: float = 8.0  # GB
    ):
        self.num_tiles = num_tiles
        self.strategy = strategy
        self.memory_limit_per_tile = memory_limit_per_tile
        self._devices_created = []
        
        # åˆ›å»ºè™šæ‹Ÿè®¾å¤‡
        self._create_mock_devices()
        
        # Llama æ¨¡å‹é…ç½®æ¨¡æ¿
        self.llama_configs = {
            "llama-7b": {"num_layers": 32, "hidden_size": 4096, "intermediate_size": 11008},
            "llama-13b": {"num_layers": 40, "hidden_size": 5120, "intermediate_size": 13824},
            "llama-33b": {"num_layers": 60, "hidden_size": 6656, "intermediate_size": 17920},
        }
    
    def _create_mock_devices(self):
        """åˆ›å»ºæ¨¡æ‹Ÿå…‰å­è®¡ç®—è®¾å¤‡"""
        try:
            import lumina_kernel
            
            for i in range(self.num_tiles):
                device_name = f"Lumina-Tile-{i}"
                self._devices_created.append(device_name)
                print(f"ğŸ“± åˆ›å»ºå…‰å­è®¡ç®—ç“¦ç‰‡: {device_name}")
                
        except ImportError:
            print("âš ï¸  lumina_kernel æœªæ‰¾åˆ°ï¼Œä½¿ç”¨è™šæ‹Ÿè®¾å¤‡")
    
    def analyze_model_complexity(self, model_config: Dict[str, Any]) -> List[LayerProfile]:
        """åˆ†ææ¨¡å‹å¤æ‚åº¦"""
        profiles = []
        num_layers = model_config["num_layers"]
        hidden_size = model_config["hidden_size"] 
        intermediate_size = model_config["intermediate_size"]
        
        for layer_idx in range(num_layers):
            # è®¡ç®—æ¯ä¸ªå±‚çš„è®¡ç®—å¤æ‚åº¦
            attention_compute = hidden_size ** 2  # è‡ªæ³¨æ„åŠ›è®¡ç®—
            ffn_compute = hidden_size * intermediate_size  # å‰é¦ˆç½‘ç»œè®¡ç®—
            total_compute = attention_compute + ffn_compute
            
            # ä¼°ç®—å†…å­˜éœ€æ±‚
            attention_memory = hidden_size ** 2 * 4 / (1024**2)  # MB
            ffn_memory = hidden_size * intermediate_size * 4 / (1024**2)  # MB
            total_memory = attention_memory + ffn_memory
            
            # å…‰å­è®¡ç®—æ•ˆç‡ï¼ˆåŸºäºè®¡ç®—æ¨¡å¼ï¼‰
            photonic_efficiency = self._estimate_photonic_efficiency(
                attention_compute, ffn_compute
            )
            
            profile = LayerProfile(
                layer_idx=layer_idx,
                layer_type="transformer",
                compute_units=total_compute / 1e6,  # æ ‡å‡†åŒ–
                memory_mb=total_memory,
                photonic_efficiency=photonic_efficiency,
                dependencies=[layer_idx - 1] if layer_idx > 0 else []
            )
            profiles.append(profile)
        
        return profiles
    
    def _estimate_photonic_efficiency(self, attention_compute: float, ffn_compute: float) -> float:
        """ä¼°ç®—å…‰å­è®¡ç®—æ•ˆç‡"""
        # çŸ©é˜µä¹˜æ³•åœ¨å…‰å­è®¡ç®—ä¸Šæ•ˆç‡æ›´é«˜
        total_matrix_ops = attention_compute + ffn_compute
        matrix_ratio = total_matrix_ops / (total_matrix_ops + 1e6)
        
        # è€ƒè™‘ç›¸å¹²æ€§ç­‰å› ç´ çš„å½±å“
        coherence_factor = 0.85  # å…‰å­ç›¸å¹²æ€§æ•ˆç‡
        noise_factor = 0.90      # å™ªå£°å®¹å¿åº¦
        
        return matrix_ratio * coherence_factor * noise_factor
    
    def partition_model(
        self, 
        model_name: str,
        model_config: Dict[str, Any]
    ) -> List[TileAssignment]:
        """åˆ†å‰²æ¨¡å‹åˆ°å¤šä¸ªç“¦ç‰‡"""
        print(f"ğŸ”§ å¼€å§‹åˆ†å‰² {model_name} åˆ° {self.num_tiles} ä¸ªå…‰å­ç“¦ç‰‡")
        print(f"ğŸ“Š ä½¿ç”¨ç­–ç•¥: {self.strategy.value}")
        
        # åˆ†ææ¨¡å‹å¤æ‚åº¦
        profiles = self.analyze_model_complexity(model_config)
        
        # æ ¹æ®ç­–ç•¥è¿›è¡Œåˆ†å‰²
        if self.strategy == PartitionStrategy.BY_LAYERS:
            assignments = self._partition_by_layers(profiles)
        elif self.strategy == PartitionStrategy.BY_COMPUTE:
            assignments = self._partition_by_compute(profiles)
        elif self.strategy == PartitionStrategy.BY_MEMORY:
            assignments = self._partition_by_memory(profiles)
        else:  # HYBRID
            assignments = self._partition_hybrid(profiles)
        
        # éªŒè¯åˆ†å‰²ç»“æœ
        self._validate_assignments(assignments, profiles)
        
        return assignments
    
    def _partition_by_layers(self, profiles: List[LayerProfile]) -> List[TileAssignment]:
        """æŒ‰å±‚æ•°å‡åŒ€åˆ†å‰²"""
        assignments = []
        layers_per_tile = len(profiles) // self.num_tiles
        remainder = len(profiles) % self.num_tiles
        
        start_idx = 0
        for tile_idx in range(self.num_tiles):
            # åˆ†é…å±‚æ•°ï¼Œä½™æ•°åˆ†é…åˆ°å‰é¢çš„ç“¦ç‰‡
            num_layers = layers_per_tile + (1 if tile_idx < remainder else 0)
            end_idx = start_idx + num_layers
            
            tile_layers = list(range(start_idx, end_idx))
            
            # è®¡ç®—æ€»è´Ÿè½½
            total_compute = sum(profiles[i].compute_units for i in tile_layers)
            total_memory = sum(profiles[i].memory_mb for i in tile_layers)
            
            assignment = TileAssignment(
                tile_id=f"Tile-{tile_idx}",
                device_id=f"Lumina-Tile-{tile_idx}",
                layers=tile_layers,
                total_compute=total_compute,
                total_memory=total_memory,
                estimated_time=self._estimate_execution_time(total_compute)
            )
            assignments.append(assignment)
            start_idx = end_idx
        
        return assignments
    
    def _partition_by_compute(self, profiles: List[LayerProfile]) -> List[TileAssignment]:
        """æŒ‰è®¡ç®—å¤æ‚åº¦åˆ†å‰²"""
        total_compute = sum(p.compute_units for p in profiles)
        target_compute_per_tile = total_compute / self.num_tiles
        
        assignments = []
        current_tile_layers = []
        current_compute = 0
        tile_idx = 0
        
        for i, profile in enumerate(profiles):
            current_tile_layers.append(i)
            current_compute += profile.compute_units
            
            # å¦‚æœå½“å‰ç“¦ç‰‡è®¡ç®—é‡æ¥è¿‘ç›®æ ‡ï¼Œå¼€å§‹ä¸‹ä¸€ä¸ªç“¦ç‰‡
            if (current_compute >= target_compute_per_tile or 
                i == len(profiles) - 1 or
                tile_idx == self.num_tiles - 1):
                
                total_memory = sum(profiles[j].memory_mb for j in current_tile_layers)
                
                assignment = TileAssignment(
                    tile_id=f"Tile-{tile_idx}",
                    device_id=f"Lumina-Tile-{tile_idx}",
                    layers=current_tile_layers.copy(),
                    total_compute=current_compute,
                    total_memory=total_memory,
                    estimated_time=self._estimate_execution_time(current_compute)
                )
                assignments.append(assignment)
                
                current_tile_layers.clear()
                current_compute = 0
                tile_idx += 1
        
        return assignments
    
    def _partition_by_memory(self, profiles: List[LayerProfile]) -> List[TileAssignment]:
        """æŒ‰å†…å­˜éœ€æ±‚åˆ†å‰²"""
        total_memory = sum(p.memory_mb for p in profiles)
        target_memory_per_tile = min(
            total_memory / self.num_tiles,
            self.memory_limit_per_tile * 1024  # GB to MB
        )
        
        assignments = []
        current_tile_layers = []
        current_memory = 0
        tile_idx = 0
        
        for i, profile in enumerate(profiles):
            # æ£€æŸ¥å†…å­˜é™åˆ¶
            if current_memory + profile.memory_mb > target_memory_per_tile:
                # ä¿å­˜å½“å‰ç“¦ç‰‡
                total_compute = sum(profiles[j].compute_units for j in current_tile_layers)
                
                assignment = TileAssignment(
                    tile_id=f"Tile-{tile_idx}",
                    device_id=f"Lumina-Tile-{tile_idx}",
                    layers=current_tile_layers.copy(),
                    total_compute=total_compute,
                    total_memory=current_memory,
                    estimated_time=self._estimate_execution_time(total_compute)
                )
                assignments.append(assignment)
                
                current_tile_layers.clear()
                current_memory = 0
                tile_idx += 1
            
            current_tile_layers.append(i)
            current_memory += profile.memory_mb
        
        # å¤„ç†æœ€åä¸€ä¸ªç“¦ç‰‡
        if current_tile_layers and len(assignments) < self.num_tiles:
            total_compute = sum(profiles[j].compute_units for j in current_tile_layers)
            
            assignment = TileAssignment(
                tile_id=f"Tile-{tile_idx}",
                device_id=f"Lumina-Tile-{tile_idx}",
                layers=current_tile_layers,
                total_compute=total_compute,
                total_memory=current_memory,
                estimated_time=self._estimate_execution_time(total_compute)
            )
            assignments.append(assignment)
        
        return assignments
    
    def _partition_hybrid(self, profiles: List[LayerProfile]) -> List[TileAssignment]:
        """æ··åˆç­–ç•¥åˆ†å‰²ï¼ˆæ¨èï¼‰"""
        # 1. é¦–å…ˆæŒ‰å†…å­˜è¿›è¡Œåˆæ­¥åˆ†å‰²ç¡®ä¿å†…å­˜é™åˆ¶
        memory_assignments = self._partition_by_memory(profiles)
        
        # 2. ç„¶ååœ¨å†…å­˜é™åˆ¶å†…ä¼˜åŒ–è®¡ç®—è´Ÿè½½
        assignments = self._balance_compute_load(memory_assignments, profiles)
        
        return assignments
    
    def _balance_compute_load(
        self, 
        initial_assignments: List[TileAssignment],
        profiles: List[LayerProfile]
    ) -> List[TileAssignment]:
        """åœ¨å†…å­˜é™åˆ¶å†…å¹³è¡¡è®¡ç®—è´Ÿè½½"""
        # è®¡ç®—å¹³å‡è®¡ç®—è´Ÿè½½
        total_compute = sum(a.total_compute for a in initial_assignments)
        target_compute = total_compute / len(initial_assignments)
        
        assignments = initial_assignments.copy()
        
        # ç®€å•çš„è´Ÿè½½å†å¹³è¡¡ç®—æ³•
        for _ in range(3):  # å¤šæ¬¡è¿­ä»£ä¼˜åŒ–
            for i in range(len(assignments)):
                for j in range(len(assignments)):
                    if i != j and assignments[i].total_compute > target_compute * 1.2:
                        # å°è¯•ç§»åŠ¨å±‚åˆ°è´Ÿè½½è¾ƒè½»çš„ç“¦ç‰‡
                        for layer_idx in assignments[i].layers.copy():
                            profile = profiles[layer_idx]
                            
                            # æ£€æŸ¥ç›®æ ‡ç“¦ç‰‡æ˜¯å¦æœ‰è¶³å¤Ÿå†…å­˜
                            if (assignments[j].total_memory + profile.memory_mb <= 
                                self.memory_limit_per_tile * 1024):
                                
                                # ç§»åŠ¨å±‚
                                assignments[i].layers.remove(layer_idx)
                                assignments[j].layers.append(layer_idx)
                                
                                # æ›´æ–°è´Ÿè½½è®¡ç®—
                                assignments[i].total_compute -= profile.compute_units
                                assignments[i].total_memory -= profile.memory_mb
                                assignments[j].total_compute += profile.compute_units
                                assignments[j].total_memory += profile.memory_mb
                                
                                break
        
        # é‡æ–°è®¡ç®—æ‰§è¡Œæ—¶é—´
        for assignment in assignments:
            assignment.estimated_time = self._estimate_execution_time(assignment.total_compute)
        
        return assignments
    
    def _estimate_execution_time(self, compute_units: float) -> float:
        """ä¼°ç®—æ‰§è¡Œæ—¶é—´ (ms)"""
        # å‡è®¾å…‰å­è®¡ç®—åŠ é€Ÿæ¯”ä¸º 5x
        photonic_speedup = 5.0
        base_performance = 1000.0  # åŸºå‡†æ€§èƒ½ (compute_units/ms)
        
        return compute_units / (base_performance * photonic_speedup)
    
    def _validate_assignments(
        self, 
        assignments: List[TileAssignment],
        profiles: List[LayerProfile]
    ):
        """éªŒè¯åˆ†å‰²ç»“æœ"""
        print("\nâœ… åˆ†å‰²éªŒè¯:")
        
        # æ£€æŸ¥å±‚æ•°å®Œæ•´æ€§
        all_layers = []
        for assignment in assignments:
            all_layers.extend(assignment.layers)
        all_layers.sort()
        
        expected_layers = list(range(len(profiles)))
        if all_layers != expected_layers:
            raise ValueError(f"å±‚æ•°ä¸åŒ¹é…: æœŸæœ› {expected_layers}, å®é™… {all_layers}")
        
        # æ£€æŸ¥å†…å­˜é™åˆ¶
        memory_violations = 0
        for assignment in assignments:
            if assignment.total_memory > self.memory_limit_per_tile * 1024:
                memory_violations += 1
        
        if memory_violations > 0:
            print(f"âš ï¸  {memory_violations} ä¸ªç“¦ç‰‡è¶…å‡ºå†…å­˜é™åˆ¶")
        else:
            print("âœ… æ‰€æœ‰ç“¦ç‰‡å†…å­˜ä½¿ç”¨æ­£å¸¸")
        
        # è®¡ç®—è´Ÿè½½å‡è¡¡åº¦
        compute_loads = [a.total_compute for a in assignments]
        avg_load = sum(compute_loads) / len(compute_loads)
        load_variance = sum((load - avg_load) ** 2 for load in compute_loads) / len(compute_loads)
        load_std = math.sqrt(load_variance)
        load_balance_ratio = 1.0 - (load_std / avg_load) if avg_load > 0 else 0
        
        print(f"ğŸ“Š è´Ÿè½½å‡è¡¡åº¦: {load_balance_ratio:.2%}")
        print(f"â±ï¸  å¹³å‡æ‰§è¡Œæ—¶é—´: {avg_load / 1000:.2f} ms")
    
    def print_partition_summary(self, assignments: List[TileAssignment]):
        """æ‰“å°åˆ†å‰²æ‘˜è¦"""
        print(f"\nğŸ¯ åˆ†å¸ƒå¼åˆ†å‰²æ‘˜è¦:")
        print("=" * 80)
        
        total_time = max(a.estimated_time for a in assignments)
        
        for i, assignment in enumerate(assignments):
            print(f"\nğŸ“± {assignment.tile_id} ({assignment.device_id})")
            print(f"   å±‚èŒƒå›´: {min(assignment.layers)}-{max(assignment.layers)} ({len(assignment.layers)} å±‚)")
            print(f"   è®¡ç®—è´Ÿè½½: {assignment.total_compute:.1f} units")
            print(f"   å†…å­˜éœ€æ±‚: {assignment.total_memory:.1f} MB ({assignment.total_memory/1024:.2f} GB)")
            print(f"   é¢„ä¼°æ—¶é—´: {assignment.estimated_time:.2f} ms")
            print(f"   è´Ÿè½½å æ¯”: {assignment.total_compute / sum(a.total_compute for a in assignments) * 100:.1f}%")
        
        print(f"\nâš¡ æ€»ä½“æ€§èƒ½:")
        print(f"   å¹¶è¡ŒåŠ é€Ÿæ¯”: ~{len(assignments)}x")
        print(f"   ç“¶é¢ˆæ—¶é—´: {total_time:.2f} ms")
        print(f"   å†…å­˜æ•ˆç‡: {sum(a.total_memory for a in assignments) / 1024:.2f} GB")