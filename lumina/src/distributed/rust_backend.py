"""
Rust åç«¯é›†æˆæ¨¡å—

æä¾›ä¸ Rust lumina_kernel çš„é›†æˆæ¥å£ï¼Œç”¨äºçœŸæ­£çš„å…‰å­è®¡ç®—åŠ é€Ÿ
"""

import torch
import numpy as np
import time
from typing import Optional, Tuple, Dict, Any
import lumina_kernel


class RustPhotonicExecutor:
    """
    Rust åç«¯å…‰å­è®¡ç®—æ‰§è¡Œå™¨

    ä½¿ç”¨ lumina_kernel Rust æ¨¡å—æ‰§è¡ŒçœŸæ­£çš„å…‰å­çŸ©é˜µè¿ç®—
    """

    def __init__(
        self,
        device_name: Optional[str] = None,
        noise_std: float = 0.01,
        bits: int = 8,
        enable_noise: bool = True
    ):
        """
        Args:
            device_name: è®¾å¤‡åç§° (None=é»˜è®¤è®¾å¤‡)
            noise_std: å™ªå£°æ ‡å‡†å·®
            bits: é‡åŒ–ä½æ•°
            enable_noise: æ˜¯å¦å¯ç”¨å™ªå£°æ³¨å…¥
        """
        self.device_name = device_name
        self.noise_std = noise_std
        self.bits = bits
        self.enable_noise = enable_noise

        # ç¡®ä¿è®¾å¤‡å·²åˆ›å»º
        if device_name and device_name not in lumina_kernel.list_devices():
            lumina_kernel.create_mock_device(device_name, 8 * 1024**3)

        self.stats = {
            "total_layers": 0,
            "total_time": 0.0,
            "avg_layer_time": 0.0
        }
        print(f"ğŸš€ RustPhotonicExecutor åˆå§‹åŒ–å®Œæˆ")
        print(f"   è®¾å¤‡: {device_name or 'default'}")
        print(f"   å™ªå£°: {'å¯ç”¨' if enable_noise else 'ç¦ç”¨'} ({noise_std})")
        print(f"   é‡åŒ–: {bits} ä½")

    def execute_layer(
        self,
        input_tensor: torch.Tensor,
        weight: torch.Tensor,
        bias: Optional[torch.Tensor] = None,
        physics_params: Optional[Dict[str, float]] = None
    ) -> Tuple[torch.Tensor, float]:
        """
        åœ¨å…‰å­ç¡¬ä»¶ä¸Šæ‰§è¡Œå•å±‚è®¡ç®—

        Args:
            input_tensor: è¾“å…¥å¼ é‡ [batch, in_features]
            weight: æƒé‡çŸ©é˜µ [out_features, in_features]
            bias: å¯é€‰åç½® [out_features]
            physics_params: ç‰©ç†ä»¿çœŸå‚æ•° (thermal_crosstalk, optical_loss_db, temperature)

        Returns:
            output_tensor: è¾“å‡ºå¼ é‡ [batch, out_features]
            execution_time: æ‰§è¡Œæ—¶é—´ (ç§’)
        """
        start_time = time.time()

        # è½¬æ¢ä¸º numpy (Rust API ç›´æ¥æ¥å— numpy æ•°ç»„)
        input_np = input_tensor.detach().cpu().numpy()
        weight_np = weight.detach().cpu().numpy()
        # Rust API çš„ bias å‚æ•°æ˜¯å¯é€‰çš„ï¼Œä¼ å…¥ None è¡¨ç¤ºä¸éœ€è¦åç½®
        bias_np = bias.detach().cpu().numpy() if bias is not None else None

        # è°ƒç”¨ Rust åç«¯
        if physics_params:
            try:
                # å°è¯•è°ƒç”¨å¢å¼ºçš„ç‰©ç†ä»¿çœŸæ¥å£
                output_np = lumina_kernel.optical_linear_physics(
                    input_np,
                    weight_np,
                    bias_np,
                    physics_params,
                    self.bits,
                    seed=42
                )
                exec_time = time.time() - start_time
            except AttributeError:
                # å›é€€åˆ°æ ‡å‡†æ¥å£ (å¦‚æœ Rust å†…æ ¸æœªæ›´æ–°)
                if self.enable_noise:
                    output_np = lumina_kernel.optical_linear_fused(
                        input_np, weight_np, bias_np, self.noise_std, self.bits, seed=42
                    )
                else:
                    output_np = lumina_kernel.optical_linear_infer(
                        input_np, weight_np, bias_np, self.bits
                    )
                exec_time = time.time() - start_time
        elif self.enable_noise:
            output_np = lumina_kernel.optical_linear_fused(
                input_np,
                weight_np,
                bias_np,
                self.noise_std,
                self.bits,
                seed=42
            )
            exec_time = time.time() - start_time
        else:
            output_np = lumina_kernel.optical_linear_infer(
                input_np,
                weight_np,
                bias_np,
                self.bits
            )
            exec_time = time.time() - start_time

        # è½¬æ¢å› torch å¼ é‡
        output_tensor = torch.from_numpy(output_np).to(input_tensor.device)

        # æ›´æ–°ç»Ÿè®¡
        total_time = time.time() - start_time
        self.stats["total_layers"] += 1
        self.stats["total_time"] += total_time
        self.stats["avg_layer_time"] = self.stats["total_time"] / self.stats["total_layers"]

        return output_tensor, total_time

    def execute_layer_inference(
        self,
        input_tensor: torch.Tensor,
        weight: torch.Tensor,
        bias: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, float]:
        """
        æ‰§è¡Œæ¨ç†ï¼ˆæ— å™ªå£°ï¼‰

        Args:
            input_tensor: è¾“å…¥å¼ é‡
            weight: æƒé‡çŸ©é˜µ
            bias: å¯é€‰åç½®

        Returns:
            output_tensor, æ‰§è¡Œæ—¶é—´
        """
        old_noise = self.enable_noise
        self.enable_noise = False
        try:
            return self.execute_layer(input_tensor, weight, bias)
        finally:
            self.enable_noise = old_noise

    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ‰§è¡Œç»Ÿè®¡"""
        return {
            "total_layers": self.stats["total_layers"],
            "total_time": self.stats["total_time"],
            "avg_layer_time": self.stats["avg_layer_time"],
            "throughput": self.stats["total_layers"] / self.stats["total_time"] if self.stats["total_time"] > 0 else 0
        }

    def print_stats(self):
        """æ‰“å°æ‰§è¡Œç»Ÿè®¡"""
        stats = self.get_stats()
        print(f"\nğŸ“Š Rust åç«¯æ‰§è¡Œç»Ÿè®¡:")
        print(f"   æ€»å±‚æ•°: {stats['total_layers']}")
        print(f"   æ€»æ—¶é—´: {stats['total_time']:.3f}s")
        print(f"   å¹³å‡å±‚æ—¶é—´: {stats['avg_layer_time']*1000:.2f}ms")
        print(f"   ååé‡: {stats['throughput']:.1f} layers/s")


class HybridExecutor:
    """
    æ··åˆæ‰§è¡Œå™¨

    æ™ºèƒ½é€‰æ‹©ä½¿ç”¨ Rust åç«¯æˆ– Python æ¨¡æ‹Ÿ
    - é¦–æ¬¡è¿è¡Œæˆ–å°æ‰¹é‡: Rust åç«¯
    - å¤§æ‰¹é‡æˆ–ç®¡é“æ¨¡å¼: æµæ°´çº¿ä¼˜åŒ–
    """

    def __init__(
        self,
        use_rust: bool = True,
        **kwargs
    ):
        self.use_rust = use_rust
        self.rust_executor: Optional[RustPhotonicExecutor] = None
        self.kwargs = kwargs

        if use_rust:
            try:
                self.rust_executor = RustPhotonicExecutor(**kwargs)
            except Exception as e:
                print(f"âš ï¸  Rust åç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                print("   å›é€€åˆ°æ¨¡æ‹Ÿæ¨¡å¼")
                self.use_rust = False

    def execute_layer(
        self,
        input_tensor: torch.Tensor,
        weight: torch.Tensor,
        bias: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, float]:
        """æ‰§è¡Œå±‚è®¡ç®—"""
        if self.use_rust and self.rust_executor:
            return self.rust_executor.execute_layer(input_tensor, weight, bias)
        else:
            # æ¨¡æ‹Ÿæ‰§è¡Œ
            start_time = time.time()
            output = torch.nn.functional.linear(input_tensor, weight, bias)
            exec_time = time.time() - start_time
            return output, exec_time

    def get_backend_type(self) -> str:
        """è·å–åç«¯ç±»å‹"""
        return "Rust (Photonic)" if self.use_rust else "Python (Simulation)"


def benchmark_executor(
    executor: HybridExecutor,
    num_layers: int = 12,
    batch_size: int = 2,
    hidden_size: int = 4096
) -> Dict[str, Any]:
    """
    åŸºå‡†æµ‹è¯•æ‰§è¡Œå™¨

    Args:
        executor: æ‰§è¡Œå™¨å®ä¾‹
        num_layers: å±‚æ•°
        batch_size: æ‰¹æ¬¡å¤§å°
        hidden_size: éšè—ç»´åº¦

    Returns:
        æ€§èƒ½æŒ‡æ ‡å­—å…¸
    """
    print(f"\nğŸ”¬ åŸºå‡†æµ‹è¯•: {executor.get_backend_type()}")
    print(f"   å±‚æ•°: {num_layers}, æ‰¹æ¬¡: {batch_size}, éšè—: {hidden_size}")

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    weights = [
        torch.randn(hidden_size, hidden_size, requires_grad=False)
        for _ in range(num_layers)
    ]

    # é¢„çƒ­
    print("   é¢„çƒ­...")
    test_input = torch.randn(batch_size, hidden_size)
    for w in weights[:2]:
        _ = executor.execute_layer(test_input, w)

    # æ­£å¼æµ‹è¯•
    print("   æ‰§è¡Œæµ‹è¯•...")
    start_time = time.time()
    layer_times = []

    input_tensor = test_input
    for i, w in enumerate(weights):
        output, exec_time = executor.execute_layer(input_tensor, w)
        layer_times.append(exec_time)
        input_tensor = output

    total_time = time.time() - start_time

    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    throughput = batch_size / (total_time / num_layers)
    avg_layer_time = sum(layer_times) / len(layer_times)

    return {
        "backend": executor.get_backend_type(),
        "total_time": total_time,
        "avg_layer_time": avg_layer_time,
        "throughput": throughput,
        "layer_times": layer_times,
        "memory_efficient": True  # Rust åç«¯å†…å­˜æ•ˆç‡æ›´é«˜
    }


if __name__ == "__main__":
    print("=" * 60)
    print("Rust Backend Benchmark")
    print("=" * 60)

    # Direct test of Rust backend
    print("\nğŸ”§ Direct Rust Backend Test:")
    try:
        import numpy as np
        import torch
        import lumina_kernel

        # Create test data
        batch_size, hidden_size = 2, 4096
        input_np = np.random.randn(batch_size, hidden_size).astype(np.float32)
        weight_np = np.random.randn(hidden_size, hidden_size).astype(np.float32)

        print(f"   Input shape: {input_np.shape}")
        print(f"   Weight shape: {weight_np.shape}")

        # Warmup
        for _ in range(3):
            _ = lumina_kernel.optical_linear_fused(input_np, weight_np, None, 0.01, 8, 42)

        # Benchmark
        import time
        num_layers = 12
        start = time.time()

        for i in range(num_layers):
            output_np = lumina_kernel.optical_linear_fused(
                input_np, weight_np, None, 0.01, 8, 42 + i
            )
            input_np = output_np  # Chain the outputs

        elapsed = time.time() - start

        print(f"\nâœ… Rust Backend Results:")
        print(f"   Layers: {num_layers}")
        print(f"   Total time: {elapsed:.3f}s")
        print(f"   Avg layer time: {elapsed/num_layers*1000:.2f}ms")
        print(f"   Throughput: {num_layers/elapsed:.1f} layers/s")

    except Exception as e:
        print(f"âŒ Rust backend error: {e}")
        import traceback
        traceback.print_exc()

    print("\n" + "=" * 60)
