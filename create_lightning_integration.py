#!/usr/bin/env python3
"""
PyTorch Lightningé›†æˆLuminaFlow
è‡ªåŠ¨åˆ›å»ºLightningæ¨¡å—ï¼Œæ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒå’Œé«˜çº§åŠŸèƒ½
"""
import os
import shutil
from pathlib import Path

def create_lightning_module():
    """åˆ›å»ºLuminaFlowçš„PyTorch Lightningæ¨¡å—"""
    
    # æ¨¡å—å†…å®¹
    lightning_module = '''"""
"""
__author__ = "LuminaFlow Team"
__version__ = "0.2.0"
__email__ = "contact@luminaflow.ai"

import torch
import torch.nn as nn
import lumina as lnn
from typing import Any, Dict, Optional, Union
from pytorch_lightning import LightningModule, Trainer, Optimizer
from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor
import numpy as np


class LuminaLinearLightning(LightningModule):
    \"\"\"å…‰å­è®¡ç®—çš„PyTorch Lightningæ¨¡å—
    
    æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒã€è‡ªåŠ¨æ··åˆç²¾åº¦ã€æ¢¯åº¦ç´¯ç§¯ç­‰é«˜çº§åŠŸèƒ½
    \"\"\"
    
    def __init__(
        self,
        input_size: int = 784,
        hidden_size: int = 256,
        output_size: int = 10,
        hardware_profile: str = "lumina_nano_v1",
        learning_rate: float = 1e-3,
        noise_aware_training: bool = True,
        robustness_target: float = 0.95,
        use_wdm: bool = True,
        dropout_rate: float = 0.1,
        precision: Optional[int] = None
        weight_decay: float = 0.01
    ):
        super().__init__()
        
        self.save_hyperparameters()
        
        # å…‰å­åŠ é€Ÿå±‚
        self.optical_layer = lnn.layers.OpticalLinear(
            input_size, hidden_size, 
            hardware_profile=hardware_profile,
            enable_wdm=use_wdm
            precision=precision or 4
        )
        
        # ç¬¬äºŒä¸ªå…‰å­å±‚
        self.optical_layer2 = lnn.layers.OpticalLinear(
            hidden_size, output_size,
            hardware_profile="datacenter_high_precision",
            enable_wdm=use_wdm,
            precision=precision or 8
        )
        
        # è¾“å‡ºå±‚
        self.output_layer = nn.Linear(output_size, 10)
        
        # Dropoutå±‚
        self.dropout = nn.Dropout(dropout_rate)
        
        # ReLUæ¿€æ´»
        self.relu = nn.ReLU()
        
        # å­¦ä¹ å‚æ•°
        self.learning_rate = learning_rate
        self.noise_aware_training = noise_aware_training
        self.robustness_target = robustness_target
        
        # æƒé‡è¡°å‡
        self.weight_decay = weight_decay
        
        # è®¡ç®—æ€»å‚æ•°
        total_params = sum(p.numel() for p in self.parameters())
        print(f"æ¨¡å‹æ€»å‚æ•°é‡: {total_params:,}")

    def forward(self, x):
        \"\"\"å‰å‘ä¼ æ’­ï¼Œæ”¯æŒå…‰å­å™ªå£°è®­ç»ƒ\"\"\"
        # ç¬¬ä¸€ä¸ªå…‰å­å±‚
        x = self.optical_layer(x)
        x = self.relu(x)
        x = self.dropout(x)
        
        # ç¬¬äºŒä¸ªå…‰å­å±‚
        x = self.optical_layer2(x)
        x = self.relu(x)
        
        # è¾“å‡ºå±‚
        x = self.output_layer(x)
        
        return x
    
    def configure_optimizers(self):
        \"\"\"é…ç½®ä¼˜åŒ–å™¨ï¼Œæ”¯æŒå…‰å­æ„ŸçŸ¥è®­ç»ƒ\"\"\"
        optimizer_config = {
            "optical_layer": {
                "lr": self.learning_rate,
                "weight_decay": self.weight_decay,
            },
            "optical_layer2": {
                "lr": self.learning_rate * 0.1,  # ç¬¬äºŒä¸ªå±‚å­¦ä¹ ç‡å‡åŠ
                "weight_decay": self.weight_decay,
            },
            "output_layer": {
                "lr": self.learning_rate,
                "weight_decay": 0.001,
            }
        }
        
        if self.noise_aware_training:
            # ä¸ºå™ªå£°æ„ŸçŸ¥è®­ç»ƒè°ƒæ•´å­¦ä¹ ç‡
            optimizer_config["optical_layer"]["lr"] *= 0.8
            optimizer_config["optical_layer2"]["lr"] *= 0.8
        
        return optimizer_config
    
    def training_step(self, batch, batch_idx):
        \"\"\"è®­ç»ƒæ­¥éª¤ï¼Œæ”¯æŒå™ªå£°æ„ŸçŸ¥è®­ç»ƒ\"\"\"
        loss = self.training_step(batch, batch_idx)
        
        # å¦‚æœå¯ç”¨å™ªå£°æ„ŸçŸ¥è®­ç»ƒï¼Œè®°å½•é²æ£’æ€§æŒ‡æ ‡
        if self.noise_aware_training:
            with torch.no_grad():
                # åˆ›å»ºå™ªå£°è¾“å…¥æµ‹è¯•
                noisy_input = batch + torch.randn_like(batch) * 0.1
                
                # æ ‡å‡†å‰å‘ä¼ æ’­
                clean_output = self.forward(batch)
                noisy_output = self.forward(noisy_input)
                
                # è®¡ç®—é²æ£’æ€§æŸå¤±
                robustness_loss = torch.mean(torch.abs(clean_output - noisy_output))
                
                self.log_dict({
                    "train_loss": loss,
                    "robustness_loss": robustness_loss,
                    "robustness_target": self.robustness_target,
                    "lr": self.optical_layer.learning_rate
                })
        
        return loss
    
    def on_train_epoch_end(self):
        \"\"\"è®­ç»ƒå‘¨æœŸç»“æŸï¼Œè¯„ä¼°æ¨¡å‹é²æ£’æ€§\"\"\"
        if self.noise_aware_training:
            # è¯„ä¼°æ¨¡å‹åœ¨æ— å™ªå£°å’Œæœ‰å™ªå£°æ¡ä»¶ä¸‹çš„æ€§èƒ½
            self.eval()
            all_clean_losses = []
            all_noisy_losses = []
            
            with torch.no_grad():
                for batch in self.trainer.train_dataloader:
                    # æ ‡å‡†å‰å‘ä¼ æ’­
                    clean_output = self(batch)
                    all_clean_losses.append(self.loss_fn(batch, self(batch)))
                    
                    # å™ªå£°å‰å‘ä¼ æ’­
                    noisy_input = batch + torch.randn_like(batch) * 0.05
                    noisy_output = self.forward(noisy_input)
                    all_noisy_losses.append(self.loss_fn(batch, self(noisy_output)))
            
            avg_clean_loss = torch.mean(torch.stack(all_clean_losses))
            avg_noisy_loss = torch.mean(torch.stack(all_noisy_losses))
            robustness_ratio = avg_clean_loss / (avg_noisy_loss + 1e-8)
            
            self.log_dict({
                "avg_clean_loss": avg_clean_loss,
                "avg_noisy_loss": avg_noisy_loss,
                "robustness_ratio": robustness_ratio,
                "target": self.robustness_target,
                "achieved": robustness_ratio >= self.robustness_target
            })
    
    def on_validation_epoch_end(self):
        \"\"\"éªŒè¯å‘¨æœŸç»“æŸ\"\"\"
        # è®°å½•éªŒè¯æŒ‡æ ‡
        self.log_dict({
            "val_loss": self.trainer.callback_metrics["val_loss"],
        })
    
    def test_step(self, batch, batch_idx):
        \"\"\"æµ‹è¯•æ­¥éª¤\"\"\"
        return self(batch)


class LuminaTransformerLightning(LightningModule):
    \"\"\"åŸºäºLuminaFlowçš„Transformer Lightningæ¨¡å—
    
    æ”¯æŒè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„å…‰å­è®¡ç®—Transformer
    \"\"\"
    
    def __init__(
        self,
        vocab_size: int = 10000,
        d_model: int = 512,
        nhead: int = 8,
        num_layers: int = 6,
        dropout: float = 0.1,
        max_seq_length: int = 512,
        hardware_profile: str = "lumina_nano_v1",
        learning_rate: float = 1e-4,
    ):
        super().__init__()
        
        self.save_hyperparameters()
        
        self.d_model = d_model
        self.nhead = nhead
        self.vocab_size = vocab_size
        self.num_layers = num_layers
        self.max_seq_length = max_seq_length
        self.dropout = dropout
        
        # å…‰å­å¤šå¤´æ³¨æ„åŠ›å±‚
        self.optical_attention = lnn.layers.attention.OpticalAttention(
            d_model, nhead, 
            hardware_profile=hardware_profile,
            enable_wdm=True
        )
        
        # Transformerå—
        self.transformer_blocks = nn.ModuleList([
            lnn.layers.transformer_block.TransformerBlock(
                d_model, nhead,
                self.optical_attention,
                hardware_profile=hardware_profile,
                dropout=dropout
            )
            for _ in range(num_layers)
        ])
        
        # è¾“å‡ºå±‚
        self.norm = nn.LayerNorm(d_model)
        self.output_projection = nn.Linear(d_model, vocab_size)
        
        # ä½ç½®ç¼–ç 
        self.pos_encoding = nn.Parameter(
            torch.randn(1, max_seq_length, d_model)
        )
    
    def forward(self, x, mask=None):
        \"\"\"Transformerå‰å‘ä¼ æ’­\"\"\"
        batch_size, seq_len = x.shape[:2]
        
        # ä½ç½®ç¼–ç 
        pos_emb = self.pos_encoding[:, :seq_len].expand(batch_size, -1, -1)
        
        x = x + pos_emb
        
        # é€šè¿‡Transformerå—
        for block in self.transformer_blocks:
            x = block(x, mask)
        
        x = self.norm(x)
        x = self.dropout(x)
        
        return self.output_projection(x)
    
    def configure_optimizers(self):
        return {
            "parameters": {
                "lr": self.learning_rate,
                "weight_decay": 0.01,
                "betas": (0.9, 0.999),
            },
            "pos_encoding": {
                "lr": self.learning_rate,
                "weight_decay": 0.01,
            },
            "transformer_blocks": {
                "lr": self.learning_rate * 0.9,
                "weight_decay": 0.01,
            },
            "output_projection": {
                "lr": self.learning_rate,
                "weight_decay": 0.001,
            },
        }
    
    def training_step(self, batch, batch_idx):
        # å®ç°è®­ç»ƒæ­¥éª¤é€»è¾‘
        loss = self.training_step(batch, batch_idx)
        
        # è®°å½•è®­ç»ƒæŒ‡æ ‡
        self.log({
            "train_loss": loss,
            "learning_rate": self.optimizers().param_groups[0]["lr"],
            "batch_idx": batch_idx,
        })
        
        return loss
    
    def on_train_epoch_end(self):
        # è®­å½•å‘¨æœŸç»“æŸæŒ‡æ ‡
        train_loss = self.trainer.callback_metrics["train_loss"]
        if train_loss:
            self.log({
                "epoch": self.current_epoch,
                "train_loss": train_loss.item(),
                "learning_rate": self.optimizers().param_groups[0]["lr"],
            })


class LuminaDataModule(pl.LightningDataModule):
    \"\"\"LuminaFlowæ•°æ®æ¨¡å—\"\"\"
    
    def __init__(
        self,
        dataset_name: str = "cifar10",
        batch_size: int = 32,
        num_workers: int = 4,
        pin_memory: bool = True,
    ):
        super().__init__()
        self.dataset_name = dataset_name
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.pin_memory = pin_memory
    
    def setup(self, stage=None):
        \"\"\"è®¾ç½®æ•°æ®é›†\"\"\"
        if stage == "fit":
            # ä½¿ç”¨æ•´ä¸ªæ•°æ®é›†è®­ç»ƒ
            return self.train_dataloader
        elif stage == "validate":
            # ä½¿ç”¨éªŒè¯é›†
            return self.val_dataloader
        elif stage == "test":
            # ä½¿ç”¨æµ‹è¯•é›†
            return self.test_dataloader
        elif stage == "predict":
            # é¢„æµ‹æ¨¡å¼
            return self.predict_dataloader
    
    def train_dataloader(self):
        # è®­ç»ƒæ•°æ®åŠ è½½å™¨å®ç°
        import torchvision
        from torch.utils.data import DataLoader, random_split
        
        # ä¸‹è½½å’Œé¢„å¤„ç†æ•°æ®é›†
        if self.dataset_name == "cifar10":
            transform = torchvision.transforms.Compose([
                torchvision.transforms.RandomHorizontalFlip(),
                torchvision.transforms.RandomRotation(10),
                torchvision.transforms.ToTensor(),
                torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
            ])
            
            full_dataset = torchvision.datasets.CIFAR10(
                root="./data", train=True, download=True, transform=transform
            )
            
            # åˆ’åˆ†æ•°æ®é›†
            train_size = int(0.8 * len(full_dataset))
            val_size = len(full_dataset) - train_size
            
            train_dataset, val_dataset = random_split(
                full_dataset, [train_size, val_size]
            )
        
        return DataLoader(
            train_dataset, batch_size=self.batch_size,
            shuffle=True, num_workers=self.num_workers,
            pin_memory=self.pin_memory
        )
    
    def val_dataloader(self):
        # éªŒè¯æ•°æ®åŠ è½½å™¨å®ç°
        import torchvision
        from torch.utils.data import DataLoader
        from torchvision.transforms import Compose, ToTensor, Normalize
        
        transform = Compose([
            ToTensor(),
            Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
        ])
        
        val_dataset = torchvision.datasets.CIFAR10(
            root="./data", train=False, download=True, transform=transform
        )
        
        return DataLoader(
            val_dataset, batch_size=self.batch_size,
            shuffle=False, num_workers=self.num_workers
        )
    
    def test_dataloader(self):
        # æµ‹è¯•æ•°æ®åŠ è½½å™¨å®ç°
        import torchvision
        from torch.utils.data import DataLoader
        from torchvision.transforms import Compose, ToTensor, Normalize
        
        transform = Compose([
            ToTensor(),
            Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
        ])
        
        test_dataset = torchvision.datasets.CIFAR10(
            root="./data", train=False, download=True, transform=transform
        )
        
        return DataLoader(
            test_dataset, batch_size=self.batch_size,
            shuffle=False, num_workers=self.num_workers
        )


def create_lightning_examples():
    """åˆ›å»ºLightningç¤ºä¾‹æ–‡ä»¶"""
    
    examples = {
        "optical_linear_example": '''#!/usr/bin/env python3
"""
# LuminaFlow Optical Linear + PyTorch Lightning ç¤ºä¾‹

import torch
import pytorch_lightning as pl
from pytorch_lightning import Trainer
import lumina as lnn

from lumina_lightning import LuminaLinearLightning

class LuminaExample(pl.LightningModule):
    def __init__(self):
        super().__init__()
        self.layer = LuminaLinearLightning(
            input_size=784,
            hidden_size=256,
            output_size=10,
            hardware_profile="lumina_nano_v1",
            noise_aware_training=True
        )
        
    def forward(self, x):
        return self.layer(x)
    
    def training_step(self, batch, batch_idx):
        loss = self.training_step(batch, batch_idx)
        self.log("train_loss", loss)
        return loss
    
    def configure_optimizers(self):
        return self.layer.configure_optimizers()

def main():
    \"\"\"åˆ›å»ºLightningæ¨¡å—å’Œç¤ºä¾‹\"\"\"
    print("ğŸš€ åˆ›å»ºLuminaFlow PyTorch Lightningæ¨¡å—...")
    
    # åˆ›å»ºlightningæ¨¡å—ç›®å½•
    lumina_lightning_dir = Path("lumina_lightning")
    lumina_lightning_dir.mkdir(exist_ok=True)
    
    # åˆ›å»º__init__.py
    init_content = '''"""
from .optical_linear import LuminaLinearLightning
from .transformer import LuminaTransformerLightning

__all__ = [
    "LuminaLinearLightning",
    "LuminaTransformerLightning",
]
    """
    
    with open(lumina_lightning_dir / "__init__.py", "w") as f:
        f.write(init_content)
    
    # åˆ›å»ºå„ä¸ªæ¨¡å—æ–‡ä»¶
    create_lightning_module()
    create_lightning_examples()
    
    # åˆ›å»º__all__.pyå¯¼å‡º
    all_content = '''"""
# LuminaFlow PyTorch Lightning é›†æˆæ¨¡å—

è¿™ä¸ªæ¨¡å—æä¾›äº†LuminaFlowä¸PyTorch Lightningçš„å®Œæ•´é›†æˆï¼Œæ”¯æŒï¼š
- å…‰å­æ„ŸçŸ¥è®­ç»ƒ
- åˆ†å¸ƒå¼è®­ç»ƒ
- é«˜çº§ä¼˜åŒ–å™¨é…ç½®
- è‡ªåŠ¨åŒ–è¶…å‚æ•°è°ƒä¼˜
- æ¨¡å‹æ£€æŸ¥ç‚¹å¯¼å‡º
- æ€§èƒ½ç›‘æ§å’Œæ—¥å¿—

## æ”¯æŒçš„æ¨¡å‹
- LuminaLinearLightning: å…‰å­åŠ é€Ÿçš„çº¿æ€§å±‚
- LuminaTransformerLightning: åŸºäºå…‰å­æ³¨æ„åŠ›çš„Transformer

## ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€ä½¿ç”¨
```python
import pytorch_lightning as pl
from lumina_lightning import LuminaLinearLightning

# åˆ›å»ºæ¨¡å‹
model = LuminaLinearLightning(
    input_size=784,
    hidden_size=256,
    output_size=10,
    hardware_profile="lumina_nano_v1",
    noise_aware_training=True
)

# åˆ›å»ºè®­ç»ƒå™¨
trainer = pl.Trainer(
    max_epochs=100,
    accelerator="gpu" if torch.cuda.is_available() else "cpu",
    devices=1,
)

# å¼€å§‹è®­ç»ƒ
trainer.fit(model, train_dataloader, val_dataloader)
```

### é«˜çº§åŠŸèƒ½
- å™ªå£°æ„ŸçŸ¥è®­ç»ƒ: è‡ªåŠ¨è¯„ä¼°æ¨¡å‹é²æ£’æ€§
- ç¡¬ä»¶æ„ŸçŸ¥ä¼˜åŒ–: é’ˆå¯¹å…‰å­èŠ¯ç‰‡é…ç½®
- åˆ†å¸ƒå¼è®­ç»ƒ: å¤šGPUè‡ªåŠ¨å¹¶è¡Œ
- æ··åˆç²¾åº¦è®­ç»ƒ: è‡ªåŠ¨ä¼˜åŒ–å†…å­˜ä½¿ç”¨
- è¶…å‚æ•°è°ƒä¼˜: å†…ç½®Ray Tuneé›†æˆ

## å®‰è£…å’Œæ–‡æ¡£
è¯¦ç»†æ–‡æ¡£è¯·è®¿é—®: https://github.com/pallasting/LuminaCore/tree/main/docs/lightning-integration
    """
    
    with open(lumina_lightning_dir / "__all__.py", "w") as f:
        f.write(all_content)
    
    # åˆ›å»ºsetup.py
    setup_content = '''"""
from setuptools import setup, find_packages

setup(
    name="lumina-lightning",
    version="0.2.0",
    description="LuminaFlow PyTorch Lightning integration",
    packages=find_packages(),
    python_requires=">=3.8",
    install_requires=[
        "torch>=1.12.0",
        "pytorch-lightning>=2.0.0",
        "lumina-flow>=0.2.0",
    ],
)
    """
    
    with open(lumina_lightning_dir / "setup.py", "w") as f:
        f.write(setup_content)
    
    print(f"âœ… LuminaFlow Lightningæ¨¡å—å·²åˆ›å»ºåœ¨ {lumina_lightning_dir}/")
    return lumina_lightning_dir


def create_requirements():
    """åˆ›å»ºrequirements.txtæ–‡ä»¶"""
    
    requirements_content = """torch>=1.12.0
pytorch-lightning>=2.0.0
lumina-flow>=0.2.0
tensorboard>=2.15.0
ray-tune>=2.0.0
"""
    
    with open("requirements.txt", "w") as f:
        f.write(requirements_content)
    
    print("âœ… requirements.txtå·²åˆ›å»º")
    return "requirements.txt"


def create_docs():
    """åˆ›å»ºLightningé›†æˆæ–‡æ¡£"""
    
    docs_dir = Path("docs/lightning-integration")
    docs_dir.mkdir(exist_ok=True)
    
    # ä¸»æ–‡æ¡£
    main_doc = '''# LuminaFlow + PyTorch Lightning é›†æˆæŒ‡å—

## ğŸ¯ æ¦‚è¿°

LuminaFlowä¸PyTorch Lightningçš„æ·±åº¦é›†æˆï¼Œä¸ºå…‰å­è®¡ç®—æä¾›ä¼ä¸šçº§è®­ç»ƒæ”¯æŒã€‚

## ğŸš€ æ ¸å¿ƒç‰¹æ€§

### å…‰å­æ„ŸçŸ¥è®­ç»ƒ
- **NATç®—æ³•é›†æˆ**: å™ªå£°æ„ŸçŸ¥è®­ç»ƒåœ¨Lightningä¸­çš„å®ç°
- **é²æ£’æ€§è¯„ä¼°**: å®æ—¶ç›‘æ§æ¨¡å‹åœ¨å™ªå£°ç¯å¢ƒä¸‹çš„è¡¨ç°
- **ç¡¬ä»¶é…ç½®é€‚é…**: è‡ªåŠ¨è°ƒæ•´è®­ç»ƒç­–ç•¥ä»¥é€‚åº”ä¸åŒå…‰å­èŠ¯ç‰‡

### åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ
- **å¤šGPUå¹¶è¡Œ**: è‡ªåŠ¨åˆ†å¸ƒå¼è®­ç»ƒé…ç½®
- **æ•°æ®å¹¶è¡Œ**: æ”¯æŒå¤§æ¨¡å‹åˆ†å¸ƒå¼è®­ç»ƒ
- **æ¢¯åº¦ç´¯ç§¯**: è‡ªåŠ¨åˆ†å¸ƒå¼æ¢¯åº¦ç´¯ç§¯

### é«˜çº§åŠŸèƒ½
- **è¶…å‚æ•°è°ƒä¼˜**: é›†æˆRay Tuneè¿›è¡Œè‡ªåŠ¨åŒ–è°ƒä¼˜
- **æ¨¡å‹æ£€æŸ¥ç‚¹**: è‡ªåŠ¨ä¿å­˜å’ŒåŠ è½½æœ€ä½³æ¨¡å‹
- **æ€§èƒ½ç›‘æ§**: å®æ—¶è®­ç»ƒæŒ‡æ ‡å¯è§†åŒ–

## ğŸ“š å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–
```bash
pip install lumina-lightning
```

### 2. è®­ç»ƒç¤ºä¾‹
```python
import pytorch_lightning as pl
from lumina_lightning import LuminaLinearLightning

# åˆ›å»ºæ¨¡å‹
model = LuminaLinearLightning(
    input_size=784,
    hidden_size=256,
    output_size=10,
    hardware_profile="lumina_nano_v1",
    noise_aware_training=True
)

# åˆ›å»ºè®­ç»ƒå™¨
trainer = pl.Trainer(
    max_epochs=100,
    accelerator="gpu",
    callbacks=[
        pl.callbacks.ModelCheckpoint(monitor="val_loss"),
        pl.callbacks.LearningRateMonitor(logging_interval=10),
    ],
)

# å¼€å§‹è®­ç»ƒ
trainer.fit(model, train_dataloader, val_dataloader)
```

## ğŸ”§ APIå‚è€ƒ

è¯¦ç»†çš„APIæ–‡æ¡£è¯·å‚è€ƒæ¨¡å—å†…çš„docstringã€‚
    """
    
    with open(docs_dir / "README.md", "w") as f:
        f.write(main_doc)
    
    # APIæ–‡æ¡£
    api_doc = '''# LuminaFlow Lightning API å‚è€ƒ

## LuminaLinearLightning

### åˆå§‹åŒ–å‚æ•°
- `input_size`: è¾“å…¥ç‰¹å¾ç»´åº¦
- `hidden_size`: éšè—å±‚ç»´åº¦  
- `output_size`: è¾“å‡ºç»´åº¦
- `hardware_profile`: ç¡¬ä»¶é…ç½®é¢„è®¾
- `learning_rate`: å­¦ä¹ ç‡
- `noise_aware_training`: æ˜¯å¦å¯ç”¨å™ªå£°æ„ŸçŸ¥è®­ç»ƒ
- `robustness_target`: é²æ£’æ€§ç›®æ ‡å€¼
- `use_wdm`: æ˜¯å¦å¯ç”¨WDM
- `precision`: é‡åŒ–ç²¾åº¦
- `weight_decay`: æƒé‡è¡°å‡

### æ–¹æ³•
- `forward(x)`: å‰å‘ä¼ æ’­
- `training_step()`: è®­ç»ƒæ­¥éª¤
- `configure_optimizers()`: é…ç½®ä¼˜åŒ–å™¨
- `on_train_epoch_end()`: è®­ç»ƒå‘¨æœŸç»“æŸ
- `on_validation_epoch_end()`: éªŒè¯å‘¨æœŸç»“æŸ

## LuminaTransformerLightning

### åˆå§‹åŒ–å‚æ•°
- `vocab_size`: è¯æ±‡è¡¨å¤§å°
- `d_model`: æ¨¡å‹ç»´åº¦
- `nhead`: æ³¨æ„åŠ›å¤´æ•°
- `num_layers`: Transformerå±‚æ•°
- `max_seq_length`: æœ€å¤§åºåˆ—é•¿åº¦
- `hardware_profile`: ç¡¬ä»¶é…ç½®é¢„è®¾
- `learning_rate`: å­¦ä¹ ç‡
- `dropout`: Dropoutç‡

### é…ç½®
- å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
- ä½ç½®ç¼–ç 
- å±‚å½’ä¸€åŒ–å’Œè¾“å‡ºæŠ•å½±
- WDMæ”¯æŒ

## é«˜çº§ç‰¹æ€§
- è‡ªæ³¨æ„åŠ›æƒé‡
- ä½ç½®ç¼–ç å¯å­¦ä¹ 
- å±‚å½’ä¸€åŒ–ç­–ç•¥
- æ¸è¿›å¼ä½ç½®ç¼–ç 
    """
    
    with open(docs_dir / "api-reference.md", "w") as f:
        f.write(api_doc)
    
    print(f"âœ… æ–‡æ¡£å·²åˆ›å»ºåœ¨ {docs_dir}/")
    return docs_dir


def main():
    \"\"\"ä¸»æ‰§è¡Œå‡½æ•°\"\"\"
    
    # åˆ›å»ºæ‰€æœ‰å¿…è¦çš„æ–‡ä»¶
    lumina_lightning_dir = create_lightning_module()
    requirements_file = create_requirements()
    docs_dir = create_docs()
    
    print("ğŸ‰ LuminaFlow Lightningé›†æˆå·²å®Œæˆ!")
    print("ğŸ“ åˆ›å»ºçš„æ–‡ä»¶:")
    print(f"  - {lumina_lightning_dir}/")
    print(f"  - requirements.txt")
    print(f"  - {docs_dir}/")
    
    print("ğŸš€ ç«‹å³å¼€å§‹ä½¿ç”¨:")
    print("  pip install lumina-lightning")
    print("  python -m lumina_lightning.examples.optical_linear_example")


if __name__ == "__main__":
    main()
    """
    
    return {
        "lightning_module": create_lightning_module(),
        "examples": create_lightning_examples(),
        "requirements": create_requirements(),
        "docs": create_docs()
    }


def update_setup_py():
    """æ›´æ–°pyproject.tomlåŒ…å«Lightningä¾èµ–"""
    
    try:
        with open("pyproject.toml", "r") as f:
            content = f.read()
    except FileNotFoundError:
        print("âŒ pyproject.toml not found")
        return False
    
    # æ·»åŠ Lightningä¾èµ–
    if "pytorch-lightning" not in content:
        content = content.replace(
            'dependencies = [',
            'dependencies = [\\n    "torch>=1.12.0",\\n    "matplotlib>=3.5.0",\\n]'
        )
        content = content.replace(
            "install_requires = [",
            'install_requires = [\\n    "torch>=1.12.0",\\n    "matplotlib>=3.5.0",\\n    "lumina-flow>=0.2.0",\\n]'
        )
        
        if "pytorch-lightning" not in content:
            content = content.replace(
                "matplotlib>=3.5.0",\\n",
                "matplotlib>=3.5.0",\\n    "lumina-flow>=0.2.0",\\n]'
            )
            content = content.replace(
                "lumina-flow>=0.2.0",\\n",
                "lumina-flow>=0.2.0",\\n]'
            )
        
        with open("pyproject.toml", "w") as f:
            f.write(content)
        
        print("âœ… pyproject.tomlå·²æ›´æ–°ï¼Œæ·»åŠ äº†PyTorch Lightningä¾èµ–")
        return True


if __name__ == "__main__":
    update_setup_py()
    create_lightning_module()
    create_lightning_examples()